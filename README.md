# LLM-Journey: Transformer Implementation

A comprehensive PyTorch implementation of the Transformer architecture from "Attention Is All You Need" (Vaswani et al., 2017), closely following **Harvard NLP's Annotated Transformer**.

## ğŸ“š Based on Harvard NLP's Annotated Transformer

This implementation is based on the excellent [Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/) by Harvard NLP, which provides a line-by-line implementation of the original Transformer paper with detailed explanations.

### Citation

**Original Paper:**
```bibtex
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
```

**Harvard NLP's Annotated Transformer:**
- Website: https://nlp.seas.harvard.edu/annotated-transformer/
- GitHub: https://github.com/harvardnlp/annotated-transformer

## ğŸ¯ Features

- **Complete Transformer Architecture**: Full encoder-decoder implementation
- **Multi-Head Attention (MHA)**: Standard transformer attention mechanism
- **Harvard NLP Style**: Follows the annotated transformer's clean, educational structure
- **Multiple Generation Strategies**: Greedy, temperature, top-k, nucleus sampling
- **Training Infrastructure**: Learning rate scheduling, label smoothing, checkpointing
- **WikiText-2 Training**: Pre-configured for language modeling
- **Modular Design**: Easy to understand, extend, and modify
- **Backward Compatible**: Supports both Harvard NLP style and legacy API

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/LLM-Journey.git
cd LLM-Journey

# Install in editable mode
pip install -e .
```

### Usage (Harvard NLP Style - Recommended)

```python
from mha import make_model

# Create a standard Transformer (Harvard NLP way)
model = make_model(
    src_vocab=10000,  # Source vocabulary size
    tgt_vocab=10000,  # Target vocabulary size
    N=6,              # Number of layers
    d_model=512,      # Model dimension
    d_ff=2048,        # Feed-forward dimension
    h=8,              # Number of attention heads
    dropout=0.1       # Dropout probability
)

# The model is ready to use!
# It includes proper Xavier initialization
```

### Usage (Legacy Style - Still Supported)

```python
from mha import Transformer

model = Transformer(
    vocab_size=50257,      # GPT-2 vocabulary
    d_model=512,           # Model dimension
    num_heads=8,           # Attention heads
    num_encoder_layers=6,  # Encoder layers
    num_decoder_layers=6,  # Decoder layers
    d_ff=2048,             # FFN dimension
    max_seq_length=512,    # Max sequence length
    dropout=0.1            # Dropout
)
```

### Text Generation

```python
from mha.inference import TextGenerator
from transformers import GPT2Tokenizer

# Load tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Create generator
generator = TextGenerator(model, tokenizer, device='cuda')

# Generate text
text = generator.generate_greedy(
    prompt="The transformer architecture",
    max_length=50
)
print(text)

# Or use other sampling methods
text = generator.generate_with_temperature(prompt, temperature=0.8)
text = generator.generate_top_k(prompt, k=50)
text = generator.generate_nucleus(prompt, p=0.9)
```

## ğŸ“– Implementation Details

### Architecture Components

| Component | Harvard NLP Class | Description |
|-----------|------------------|-------------|
| **Model** | `EncoderDecoder` | Main seq2seq wrapper |
| **Encoder** | `Encoder` | Stack of N encoder layers |
| **Decoder** | `Decoder` | Stack of N decoder layers |
| **Attention** | `MultiHeadedAttention` | Scaled dot-product attention with h heads |
| **FFN** | `PositionwiseFeedForward` | Two-layer feed-forward network |
| **Embeddings** | `Embeddings` | Token embeddings with âˆšd_model scaling |
| **Positional Encoding** | `PositionalEncoding` | Sinusoidal position encodings |
| **Generator** | `Generator` | Linear projection + log softmax |

### Key Functions

- **`make_model()`**: Factory function to create a complete transformer
- **`attention()`**: Core scaled dot-product attention function
- **`rate()`**: Learning rate schedule with warmup
- **`greedy_decode()`**: Autoregressive text generation
- **`subsequent_mask()`**: Causal mask for decoder
- **`clones()`**: Deep copy N modules

### Training Utilities

- **`Batch`**: Batch processing with automatic masking
- **`rate()`**: Learning rate warmup schedule from the paper
- **`LabelSmoothing`**: Smoothed cross-entropy loss
- **`MetricsTracker`**: Loss and perplexity tracking
- **`CheckpointManager`**: Model checkpointing

## ğŸ”¬ Comparison: Original vs This Implementation

| Aspect | Harvard NLP Annotated Transformer | This Implementation |
|--------|-----------------------------------|---------------------|
| **Base Structure** | Single file tutorial | Modular package structure |
| **API Style** | `make_model()` factory | Both `make_model()` and `Transformer` class |
| **Architecture** | Exact paper implementation | Same + backward compatible |
| **Training** | Basic examples | Full WikiText-2 pipeline |
| **Inference** | Greedy decode | Multiple sampling strategies |
| **Organization** | Educational (single file) | Production (modular) |
| **Extras** | - | TensorBoard, checkpointing, visualization |

## ğŸ“‚ Project Structure

```
LLM-Journey/
â”œâ”€â”€ mha/                           # Main package
â”‚   â”œâ”€â”€ __init__.py               # Package exports
â”‚   â”œâ”€â”€ attention.py              # Multi-head attention
â”‚   â”œâ”€â”€ layers.py                 # LayerNorm, FFN, residual connections
â”‚   â”œâ”€â”€ positional_encoding.py   # Sinusoidal & learned PE
â”‚   â”œâ”€â”€ transformer.py            # Full architecture
â”‚   â”œâ”€â”€ utils.py                  # Training utilities
â”‚   â”œâ”€â”€ inference.py              # Text generation
â”‚   â””â”€â”€ data_loader.py            # WikiText data loading
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ train_mha_colab.ipynb    # Training notebook for Colab
â”œâ”€â”€ README.md                     # This file
â””â”€â”€ setup.py                      # Package setup
```

## ğŸ“ Training on WikiText-2

We provide a complete Colab notebook for training:

1. Open `notebooks/train_mha_colab.ipynb` in Google Colab
2. Run all cells to train on WikiText-2
3. Model checkpoints are saved to Google Drive
4. Includes inference examples and attention visualization

## ğŸ¤ Acknowledgments

This implementation is heavily inspired by and based on:

1. **Harvard NLP's Annotated Transformer** ([link](https://nlp.seas.harvard.edu/annotated-transformer/))
   - Provided the clean, educational implementation structure
   - Excellent line-by-line explanations of the paper
   - Reference implementation for `make_model()`, `attention()`, and other core functions

2. **"Attention Is All You Need"** by Vaswani et al. (2017)
   - Original Transformer paper
   - Introduced the architecture we implement here

3. **Community**: Thanks to the PyTorch and Hugging Face communities for tools and resources

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ”— Related Resources

- [Original Paper](https://arxiv.org/abs/1706.03762)
- [Harvard NLP Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)

## ğŸ“§ Contact

For questions or suggestions, please open an issue on GitHub.

---

â­ **Star this repo if you find it useful!**

Built with â¤ï¸ following Harvard NLP's excellent educational materials.
