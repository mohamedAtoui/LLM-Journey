{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Transformer Training - Harvard NLP Style (v2.0)\n",
    "## Train on WikiText-2 with Complete Monitoring & Evaluation\n",
    "\n",
    "**Based on Harvard NLP's Annotated Transformer**\n",
    "\n",
    "### üìö What's New in v2.0?\n",
    "- ‚úÖ **Harvard NLP Batch Class**: Uses `Batch` class for automatic masking\n",
    "- ‚úÖ **Extended Training**: 20 epochs (vs 3 epochs)\n",
    "- ‚úÖ **Generation Monitoring**: Sample outputs after each epoch\n",
    "- ‚úÖ **Quality Metrics**: Track repetition, diversity, coherence\n",
    "- ‚úÖ **Early Stopping**: Automatic stopping when converged\n",
    "- ‚úÖ **Better Prompts**: Wikipedia-style prompts matching training data\n",
    "- ‚úÖ **LR Visualization**: See the Harvard NLP learning rate schedule\n",
    "- ‚úÖ **Module Reload**: No more caching issues!\n",
    "- ‚úÖ **Clean Structure**: Professional organization\n",
    "- ‚úÖ **Proper Loss Calculation**: Follows Harvard NLP pattern exactly\n",
    "\n",
    "### üîë Key Harvard NLP Patterns Used:\n",
    "1. **`make_model()`** - Factory function for model creation\n",
    "2. **`Batch` class** - Automatic src/tgt splitting and masking\n",
    "3. **`rate()` scheduler** - Learning rate warmup from the paper\n",
    "4. **`EncoderDecoder`** - Main architecture wrapper\n",
    "5. **`Generator`** - Log softmax projection layer\n",
    "\n",
    "### üìã Prerequisites\n",
    "1. Upload `data_processed.zip` to Colab\n",
    "2. Extract it: `!unzip data_processed.zip`\n",
    "3. Mount Google Drive for checkpoints\n",
    "\n",
    "### ‚è±Ô∏è Expected Training Time\n",
    "- ~2.5 hours for 20 epochs on A100 GPU\n",
    "- Final perplexity: ~180-220 (vs 350+ with 3 epochs)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup & Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Check GPU & Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected - training will be VERY slow!\")\n",
    "\n",
    "print(f\"\\n‚úì Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/transformer_checkpoints_v2'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"‚úì Checkpoints will be saved to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Clone Repository & Install Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any existing installation\n",
    "!rm -rf LLM-Journey\n",
    "\n",
    "# Clone repository\n",
    "!git clone https://github.com/mohamedAtoui/LLM-Journey\n",
    "%cd LLM-Journey\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q datasets transformers tqdm matplotlib seaborn\n",
    "\n",
    "# Install mha package in editable mode\n",
    "!pip install -q -e .\n",
    "\n",
    "print(\"\\n‚úì Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4Ô∏è‚É£ Import Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Harvard NLP components\n",
    "from mha import make_model\n",
    "from mha.utils import rate, Batch  # Import Batch class!\n",
    "from mha.inference import TextGenerator\n",
    "from mha.attention import subsequent_mask\n",
    "\n",
    "print(\"‚úì All imports successful!\")\n",
    "print(\"‚úì Using Harvard NLP's Annotated Transformer patterns\")\n",
    "print(\"‚úì Training will use the Batch class for proper masking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture (Harvard NLP parameter names)\n",
    "config = {\n",
    "    # Model\n",
    "    'src_vocab': 50257,\n",
    "    'tgt_vocab': 50257,\n",
    "    'N': 6,\n",
    "    'd_model': 512,\n",
    "    'd_ff': 2048,\n",
    "    'h': 8,\n",
    "    'dropout': 0.1,\n",
    "    'max_seq_length': 512,\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': 8,\n",
    "    'num_epochs': 20,  # Extended from 3!\n",
    "    'warmup_steps': 4000,  # Increased from 2000\n",
    "    'gradient_clip': 1.0,\n",
    "    \n",
    "    # Early stopping\n",
    "    'early_stop_patience': 3,\n",
    "    'early_stop_min_delta': 0.01,\n",
    "    \n",
    "    # Dataset size (for experimental training)\n",
    "    # Set to None to use full dataset, or specify number of samples\n",
    "    'train_subset_size': 362,  # e.g., 1000 for 1k samples\n",
    "    'val_subset_size': 22,    # e.g., 200 for 200 samples\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key:20s}: {value}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Load Data\n",
    "\n",
    "### üß™ Experimental Training Options\n",
    "\n",
    "You can reduce dataset size for faster experimental training by modifying `config`:\n",
    "\n",
    "| Setup | train_subset_size | val_subset_size | Time/Epoch | Total Time (20 epochs) | Use Case |\n",
    "|-------|-------------------|-----------------|------------|------------------------|----------|\n",
    "| **Full Dataset** | `None` | `None` | ~15 min | ~5 hours | Final training |\n",
    "| **50% Dataset** | `1810` | `109` | ~7-8 min | ~2.5 hours | Quick experiment |\n",
    "| **25% Dataset** | `905` | `55` | ~4 min | ~1.3 hours | Fast iteration |\n",
    "| **10% Dataset** | `362` | `22` | ~2 min | ~40 minutes | Very quick test |\n",
    "| **5% Dataset** | `181` | `11` | ~1 min | ~20 minutes | Rapid prototyping |\n",
    "\n",
    "**Original dataset sizes:**\n",
    "- Train: 3,620 samples\n",
    "- Validation: 218 samples\n",
    "\n",
    "**Example for 10% dataset:**\n",
    "```python\n",
    "config = {\n",
    "    ...\n",
    "    'train_subset_size': 362,  # 10% of 3,620\n",
    "    'val_subset_size': 22,     # 10% of 218\n",
    "}\n",
    "```\n",
    "\n",
    "**Note:** Smaller datasets will result in:\n",
    "- ‚úÖ Much faster training\n",
    "- ‚úÖ Quick iteration for testing\n",
    "- ‚ö†Ô∏è Lower final quality (less data to learn from)\n",
    "- ‚ö†Ô∏è Higher validation perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading WikiText-2 dataset...\\n\")\n",
    "\n",
    "DATA_PATH = './data/wikitext2_processed'\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_from_disk(DATA_PATH)\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "\n",
    "# Apply subset if specified (for experimental training)\n",
    "if config['train_subset_size'] is not None:\n",
    "    train_dataset = train_dataset.select(range(min(config['train_subset_size'], len(train_dataset))))\n",
    "    print(f\"‚ö†Ô∏è Using subset of training data: {len(train_dataset)} samples\")\n",
    "\n",
    "if config['val_subset_size'] is not None:\n",
    "    val_dataset = val_dataset.select(range(min(config['val_subset_size'], len(val_dataset))))\n",
    "    print(f\"‚ö†Ô∏è Using subset of validation data: {len(val_dataset)} samples\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set PyTorch format\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "print(f\"‚úì Dataset loaded!\")\n",
    "print(f\"  Vocab size: {len(tokenizer):,}\")\n",
    "print(f\"  Train samples: {len(train_dataset):,}\")\n",
    "print(f\"  Val samples: {len(val_dataset):,}\")\n",
    "print(f\"  Sequence length: {len(train_dataset[0]['input_ids'])} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': input_ids.clone()\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"‚úì DataLoaders created\")\n",
    "print(f\"  Train batches: {len(train_loader):,}\")\n",
    "print(f\"  Val batches: {len(val_loader):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model using Harvard NLP's make_model()\n",
    "model = make_model(\n",
    "    src_vocab=config['src_vocab'],\n",
    "    tgt_vocab=config['tgt_vocab'],\n",
    "    N=config['N'],\n",
    "    d_model=config['d_model'],\n",
    "    d_ff=config['d_ff'],\n",
    "    h=config['h'],\n",
    "    dropout=config['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úì Model created!\")\n",
    "print(f\"  Architecture: EncoderDecoder (Harvard NLP)\")\n",
    "print(f\"  Layers: {config['N']} encoder + {config['N']} decoder\")\n",
    "print(f\"  Parameters: {total_params:,} ({total_params/1e6:.1f}M)\")\n",
    "print(f\"  Model size: ~{total_params * 4 / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Visualize Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Harvard NLP learning rate schedule\n",
    "steps = np.arange(1, 15000)\n",
    "lrs = [rate(s, config['d_model'], 1.0, config['warmup_steps']) for s in steps]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(steps, lrs, linewidth=2)\n",
    "plt.axvline(config['warmup_steps'], color='r', linestyle='--', \n",
    "            label=f'Warmup end ({config[\"warmup_steps\"]} steps)', linewidth=2)\n",
    "plt.xlabel('Training Step', fontsize=12)\n",
    "plt.ylabel('Learning Rate', fontsize=12)\n",
    "plt.title('Harvard NLP Learning Rate Schedule: lrate = d_model^(-0.5) * min(step^(-0.5), step * warmup^(-1.5))', \n",
    "          fontsize=13)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"LR at step 1: {rate(1, config['d_model'], 1.0, config['warmup_steps']):.8f}\")\n",
    "print(f\"LR at warmup: {rate(config['warmup_steps'], config['d_model'], 1.0, config['warmup_steps']):.6f}\")\n",
    "print(f\"LR at 2x warmup: {rate(config['warmup_steps']*2, config['d_model'], 1.0, config['warmup_steps']):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Setup Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer (parameters from the paper)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1.0,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: rate(\n",
    "        step + 1,\n",
    "        model_size=config['d_model'],\n",
    "        factor=1.0,\n",
    "        warmup=config['warmup_steps']\n",
    "    )\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.NLLLoss(ignore_index=tokenizer.pad_token_id, reduction='sum')\n",
    "\n",
    "print(\"‚úì Optimizer & loss configured\")\n",
    "print(f\"  Optimizer: Adam (betas=(0.9, 0.98), eps=1e-9)\")\n",
    "print(f\"  Scheduler: Harvard NLP rate() with {config['warmup_steps']} warmup steps\")\n",
    "print(f\"  Loss: NLLLoss (reduction='sum')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=3, min_delta=0.01):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            return False\n",
    "        \n",
    "        if val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"  ‚ö†Ô∏è EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        return False\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Format seconds into readable time\"\"\"\n",
    "    mins = int(seconds // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    return f\"{mins}m {secs}s\"\n",
    "\n",
    "print(\"‚úì Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function\n",
    "\n",
    "**Harvard NLP Pattern Explanation:**\n",
    "\n",
    "The training loop follows the Annotated Transformer's approach using the `Batch` class:\n",
    "\n",
    "```python\n",
    "# 1. Create Batch object (automatic masking)\n",
    "batch = Batch(src=input_ids, tgt=input_ids, pad=pad_token)\n",
    "# This automatically creates:\n",
    "#   - batch.src: Source sequence\n",
    "#   - batch.src_mask: Masks padding in source\n",
    "#   - batch.tgt: Target input (excludes last token)\n",
    "#   - batch.tgt_y: Target labels (excludes first token)\n",
    "#   - batch.tgt_mask: Masks padding + future tokens\n",
    "#   - batch.ntokens: Count of valid tokens\n",
    "\n",
    "# 2. Forward pass\n",
    "output = model.forward(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\n",
    "\n",
    "# 3. Generate log probabilities\n",
    "log_probs = model.generator(output)\n",
    "\n",
    "# 4. Compute loss (sum reduction)\n",
    "loss = criterion(log_probs.reshape(-1, vocab), batch.tgt_y.reshape(-1))\n",
    "\n",
    "# 5. Normalize by token count\n",
    "loss = loss / batch.ntokens\n",
    "```\n",
    "\n",
    "This is the **clean, modular approach** from Harvard NLP - no manual mask creation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, scheduler, criterion, device, epoch):\n",
    "    \"\"\"\n",
    "    Train for one epoch using Harvard NLP's Batch class pattern\n",
    "    \n",
    "    This follows the Annotated Transformer approach:\n",
    "    1. Create Batch object with automatic masking\n",
    "    2. Forward pass through model\n",
    "    3. Generate log probabilities\n",
    "    4. Compute loss (sum reduction)\n",
    "    5. Normalize by number of tokens\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    grad_norms = []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for batch_idx, data in enumerate(pbar):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        \n",
    "        # Harvard NLP pattern: Use Batch class for automatic masking\n",
    "        batch = Batch(\n",
    "            src=input_ids,\n",
    "            tgt=input_ids,\n",
    "            pad=tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        # Forward pass (Harvard NLP style)\n",
    "        decoder_output = model.forward(\n",
    "            batch.src,      # Source sequence\n",
    "            batch.tgt,      # Target input (excludes last token)\n",
    "            batch.src_mask, # Source mask (hides padding)\n",
    "            batch.tgt_mask  # Target mask (hides padding + future)\n",
    "        )\n",
    "        \n",
    "        # Generate log probabilities\n",
    "        log_probs = model.generator(decoder_output)\n",
    "        \n",
    "        # Compute loss (sum over batch)\n",
    "        loss_sum = criterion(\n",
    "            log_probs.reshape(-1, config['tgt_vocab']),\n",
    "            batch.tgt_y.reshape(-1)  # Use batch.tgt_y (excludes first token)\n",
    "        )\n",
    "        \n",
    "        # Normalize by number of tokens (Harvard NLP way)\n",
    "        num_tokens = batch.ntokens.item()  # Use batch.ntokens\n",
    "        loss = loss_sum / num_tokens if num_tokens > 0 else loss_sum\n",
    "        \n",
    "        # NaN detection\n",
    "        if not torch.isfinite(loss):\n",
    "            print(f\"\\n‚ö†Ô∏è Non-finite loss at batch {batch_idx}\")\n",
    "            continue\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), \n",
    "            config['gradient_clip']\n",
    "        )\n",
    "        grad_norms.append(grad_norm.item())\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Accumulate for epoch average\n",
    "        total_loss += loss_sum.item()\n",
    "        total_tokens += num_tokens\n",
    "        \n",
    "        # Display progress\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'lr': f\"{current_lr:.2e}\",\n",
    "            'grad': f\"{grad_norm:.2f}\"\n",
    "        })\n",
    "    \n",
    "    # Compute epoch averages\n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n",
    "    perplexity = math.exp(min(avg_loss, 100))\n",
    "    avg_grad_norm = np.mean(grad_norms) if grad_norms else 0\n",
    "    \n",
    "    return avg_loss, perplexity, avg_grad_norm\n",
    "\n",
    "print(\"‚úì Training function defined (Harvard NLP Batch class pattern)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate the model using Harvard NLP's Batch class pattern\n",
    "    \n",
    "    Same approach as training but without gradients\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for data in tqdm(val_loader, desc=\"Validation\"):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        \n",
    "        # Harvard NLP pattern: Use Batch class for automatic masking\n",
    "        batch = Batch(\n",
    "            src=input_ids,\n",
    "            tgt=input_ids,\n",
    "            pad=tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        # Forward pass\n",
    "        decoder_output = model.forward(\n",
    "            batch.src,\n",
    "            batch.tgt,\n",
    "            batch.src_mask,\n",
    "            batch.tgt_mask\n",
    "        )\n",
    "        \n",
    "        # Generate log probabilities\n",
    "        log_probs = model.generator(decoder_output)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss_sum = criterion(\n",
    "            log_probs.reshape(-1, config['tgt_vocab']),\n",
    "            batch.tgt_y.reshape(-1)\n",
    "        )\n",
    "        \n",
    "        # Accumulate\n",
    "        num_tokens = batch.ntokens.item()\n",
    "        total_loss += loss_sum.item()\n",
    "        total_tokens += num_tokens\n",
    "    \n",
    "    # Compute averages\n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n",
    "    perplexity = math.exp(min(avg_loss, 100))\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "print(\"‚úì Validation function defined (Harvard NLP Batch class pattern)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_generation(model, tokenizer, device, epoch):\n",
    "    \"\"\"Generate samples to monitor progress\"\"\"\n",
    "    model.eval()\n",
    "    generator = TextGenerator(model, tokenizer, device=device)\n",
    "    \n",
    "    # Wikipedia-style prompts (matching WikiText-2)\n",
    "    prompts = [\n",
    "        \"The Roman Empire\",\n",
    "        \"Albert Einstein\",\n",
    "        \"World War II\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Sample Generations (Epoch {epoch})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        try:\n",
    "            text = generator.generate_with_temperature(\n",
    "                prompt, temperature=0.8, max_length=40\n",
    "            )\n",
    "            print(f\"\\nPrompt: \\\"{prompt}\\\"\")\n",
    "            print(f\"Output: {text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nPrompt: \\\"{prompt}\\\"\")\n",
    "            print(f\"Error: {str(e)}\")\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    model.train()\n",
    "\n",
    "print(\"‚úì Generation evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_ppl': [],\n",
    "    'val_loss': [],\n",
    "    'val_ppl': [],\n",
    "    'grad_norm': [],\n",
    "    'epoch_time': []\n",
    "}\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=config['early_stop_patience'],\n",
    "    min_delta=config['early_stop_min_delta']\n",
    ")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "total_start_time = time.time()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total epochs: {config['num_epochs']}\")\n",
    "print(f\"Expected time: ~{config['num_epochs'] * 8.5:.0f} minutes\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for epoch in range(1, config['num_epochs'] + 1):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìÖ Epoch {epoch}/{config['num_epochs']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_ppl, grad_norm = train_epoch(\n",
    "        model, train_loader, optimizer, scheduler, criterion, device, epoch\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_ppl = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_ppl'].append(train_ppl)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_ppl'].append(val_ppl)\n",
    "    history['grad_norm'].append(grad_norm)\n",
    "    history['epoch_time'].append(epoch_time)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nüìä Results:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | PPL: {train_ppl:.2f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | PPL: {val_ppl:.2f}\")\n",
    "    print(f\"  Grad Norm:  {grad_norm:.4f}\")\n",
    "    print(f\"  Time: {format_time(epoch_time)}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        checkpoint_path = f\"{CHECKPOINT_DIR}/best_model_epoch{epoch}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_ppl': val_ppl,\n",
    "            'config': config,\n",
    "            'history': history\n",
    "        }, checkpoint_path)\n",
    "        print(f\"  ‚úÖ Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "    \n",
    "    # Sample generation every epoch\n",
    "    if epoch % 1 == 0:\n",
    "        evaluate_generation(model, tokenizer, device, epoch)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopping(val_loss):\n",
    "        print(f\"\\n‚ö†Ô∏è Early stopping triggered at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "total_time = time.time() - total_start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total time: {format_time(total_time)}\")\n",
    "print(f\"Best val loss: {best_val_loss:.4f}\")\n",
    "print(f\"Final train PPL: {history['train_ppl'][-1]:.2f}\")\n",
    "print(f\"Final val PPL: {history['val_ppl'][-1]:.2f}\")\n",
    "print(f\"\\nCheckpoints saved at: {CHECKPOINT_DIR}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Validation', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Training and Validation Loss', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Perplexity\n",
    "axes[0, 1].plot(epochs, history['train_ppl'], 'b-', label='Train', linewidth=2)\n",
    "axes[0, 1].plot(epochs, history['val_ppl'], 'r-', label='Validation', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Perplexity', fontsize=12)\n",
    "axes[0, 1].set_title('Training and Validation Perplexity', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient norm\n",
    "axes[1, 0].plot(epochs, history['grad_norm'], 'g-', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Gradient Norm', fontsize=12)\n",
    "axes[1, 0].set_title('Average Gradient Norm per Epoch', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Epoch time\n",
    "axes[1, 1].bar(epochs, history['epoch_time'], color='purple', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Time (seconds)', fontsize=12)\n",
    "axes[1, 1].set_title('Training Time per Epoch', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{CHECKPOINT_DIR}/training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Training curves saved to {CHECKPOINT_DIR}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Reload Module (Fix Caching Issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force reload mha modules to get latest code\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "print(\"Reloading mha modules...\")\n",
    "\n",
    "# Remove all mha-related modules from cache\n",
    "modules_to_remove = [key for key in sys.modules.keys() if 'mha' in key]\n",
    "for module in modules_to_remove:\n",
    "    del sys.modules[module]\n",
    "    print(f\"  Removed: {module}\")\n",
    "\n",
    "# Re-import\n",
    "from mha.inference import TextGenerator\n",
    "\n",
    "print(\"\\n‚úì Modules reloaded! Text generation will use latest code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Final Evaluation & Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Find best checkpoint\n",
    "checkpoint_files = glob.glob(f\"{CHECKPOINT_DIR}/best_model_epoch*.pt\")\n",
    "\n",
    "if checkpoint_files:\n",
    "    latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
    "    print(f\"Loading best model: {latest_checkpoint}\")\n",
    "    \n",
    "    checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Best model loaded!\")\n",
    "    print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"  Train Loss: {checkpoint['train_loss']:.4f}\")\n",
    "    print(f\"  Val Loss: {checkpoint['val_loss']:.4f}\")\n",
    "    print(f\"  Val Perplexity: {checkpoint['val_ppl']:.2f}\")\n",
    "else:\n",
    "    print(\"‚ùå No checkpoint found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive Text Generation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator with reloaded module\n",
    "generator = TextGenerator(model, tokenizer, device=device)\n",
    "\n",
    "# Wikipedia-style prompts (matching WikiText-2 training data)\n",
    "prompts = [\n",
    "    \"The Roman Empire\",\n",
    "    \"Albert Einstein was a\",\n",
    "    \"World War II began when\",\n",
    "    \"The human brain\",\n",
    "    \"Isaac Newton discovered\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ FINAL TEXT GENERATION EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"\\n{i}. Prompt: \\\"{prompt}\\\"\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    try:\n",
    "        # Greedy\n",
    "        greedy_text = generator.generate_greedy(prompt, max_length=50)\n",
    "        print(f\"  Greedy:      {greedy_text}\")\n",
    "        \n",
    "        # Temperature\n",
    "        temp_text = generator.generate_with_temperature(\n",
    "            prompt, temperature=0.8, max_length=50\n",
    "        )\n",
    "        print(f\"  Temperature: {temp_text}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì Generation test complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_generation_metrics(generator, prompts, num_samples=5):\n",
    "    \"\"\"Calculate quality metrics for generated text\"\"\"\n",
    "    metrics = {\n",
    "        'length': [],\n",
    "        'unique_tokens': [],\n",
    "        'repetition_rate': [],\n",
    "        'vocab_diversity': []\n",
    "    }\n",
    "    \n",
    "    print(\"Calculating generation quality metrics...\")\n",
    "    \n",
    "    for prompt in tqdm(prompts):\n",
    "        for _ in range(num_samples):\n",
    "            try:\n",
    "                text = generator.generate_with_temperature(\n",
    "                    prompt, temperature=0.8, max_length=50\n",
    "                )\n",
    "                tokens = text.split()\n",
    "                \n",
    "                metrics['length'].append(len(tokens))\n",
    "                metrics['unique_tokens'].append(len(set(tokens)))\n",
    "                \n",
    "                if len(tokens) > 0:\n",
    "                    repetition = 1 - (len(set(tokens)) / len(tokens))\n",
    "                    metrics['repetition_rate'].append(repetition)\n",
    "                    metrics['vocab_diversity'].append(len(set(tokens)) / len(tokens))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä GENERATION QUALITY METRICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"  Average length:      {np.mean(metrics['length']):.1f} tokens\")\n",
    "    print(f\"  Average unique:      {np.mean(metrics['unique_tokens']):.1f} tokens\")\n",
    "    print(f\"  Repetition rate:     {np.mean(metrics['repetition_rate']):.2%}\")\n",
    "    print(f\"  Vocabulary diversity: {np.mean(metrics['vocab_diversity']):.2%}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = calculate_generation_metrics(generator, prompts[:3], num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìù TRAINING SUMMARY REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüèóÔ∏è Model Architecture:\")\n",
    "print(f\"  Type: Harvard NLP EncoderDecoder\")\n",
    "print(f\"  Layers: {config['N']} √ó (Encoder + Decoder)\")\n",
    "print(f\"  Parameters: {total_params:,} ({total_params/1e6:.1f}M)\")\n",
    "print(f\"  Model dimension: {config['d_model']}\")\n",
    "print(f\"  Attention heads: {config['h']}\")\n",
    "\n",
    "print(\"\\nüìö Training Data:\")\n",
    "print(f\"  Dataset: WikiText-2\")\n",
    "print(f\"  Train samples: {len(train_dataset):,}\")\n",
    "print(f\"  Val samples: {len(val_dataset):,}\")\n",
    "print(f\"  Sequence length: 512 tokens\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Training Configuration:\")\n",
    "print(f\"  Epochs completed: {len(history['train_loss'])}\")\n",
    "print(f\"  Batch size: {config['batch_size']}\")\n",
    "print(f\"  Warmup steps: {config['warmup_steps']}\")\n",
    "print(f\"  Gradient clipping: {config['gradient_clip']}\")\n",
    "\n",
    "print(\"\\nüìä Final Results:\")\n",
    "print(f\"  Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Train PPL: {history['train_ppl'][-1]:.2f}\")\n",
    "print(f\"  Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  Val PPL: {history['val_ppl'][-1]:.2f}\")\n",
    "print(f\"  Best Val Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "improvement_loss = ((history['train_loss'][0] - history['train_loss'][-1]) / history['train_loss'][0]) * 100\n",
    "improvement_ppl = ((history['train_ppl'][0] - history['train_ppl'][-1]) / history['train_ppl'][0]) * 100\n",
    "\n",
    "print(\"\\nüìà Improvement:\")\n",
    "print(f\"  Loss reduction: {improvement_loss:.1f}%\")\n",
    "print(f\"  PPL reduction: {improvement_ppl:.1f}%\")\n",
    "\n",
    "print(\"\\n‚è±Ô∏è Training Time:\")\n",
    "print(f\"  Total: {format_time(sum(history['epoch_time']))}\")\n",
    "print(f\"  Avg per epoch: {format_time(np.mean(history['epoch_time']))}\")\n",
    "\n",
    "print(\"\\nüíæ Checkpoints:\")\n",
    "print(f\"  Location: {CHECKPOINT_DIR}\")\n",
    "print(f\"  Best model: epoch {checkpoint['epoch']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Training complete! Model ready for inference.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "### What We Achieved\n",
    "\n",
    "1. ‚úÖ **Trained a Transformer** using Harvard NLP's Annotated Transformer patterns\n",
    "2. ‚úÖ **Used Batch Class** for automatic masking (proper Harvard NLP way)\n",
    "3. ‚úÖ **Extended training** from 3 to 20 epochs for better quality\n",
    "4. ‚úÖ **Monitored progress** with generation samples after each epoch\n",
    "5. ‚úÖ **Tracked metrics**: Loss, perplexity, gradient norms, generation quality\n",
    "6. ‚úÖ **Early stopping** to prevent overfitting\n",
    "7. ‚úÖ **Saved checkpoints** to Google Drive\n",
    "\n",
    "### Harvard NLP Patterns Used\n",
    "\n",
    "This implementation strictly follows the Annotated Transformer:\n",
    "\n",
    "| Pattern | What It Does |\n",
    "|---------|-------------|\n",
    "| `make_model()` | Factory function for model creation with proper initialization |\n",
    "| `Batch` class | Automatic src/tgt splitting, masking, token counting |\n",
    "| `rate()` | Learning rate schedule: d_model^(-0.5) * min(step^(-0.5), step*warmup^(-1.5)) |\n",
    "| `EncoderDecoder` | Main architecture wrapper |\n",
    "| `Generator` | Log softmax projection layer |\n",
    "| Loss calculation | Sum reduction + normalize by token count |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- üìà **Train longer** (30-50 epochs) for even better results\n",
    "- üîß **Tune hyperparameters** (learning rate, warmup, batch size)\n",
    "- üìä **Try larger models** (8-12 layers, 768 dimensions)\n",
    "- üìö **Use more data** (WikiText-103 for better generalization)\n",
    "- üéØ **Fine-tune** on specific domains (news, scientific text, etc.)\n",
    "- üî¨ **Add label smoothing** (use `LabelSmoothing` class from mha.utils)\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Harvard NLP Annotated Transformer**: https://nlp.seas.harvard.edu/annotated-transformer/\n",
    "- **Original Paper**: \"Attention is All You Need\" (Vaswani et al., 2017)\n",
    "- **Your Implementation**: https://github.com/mohamedAtoui/LLM-Journey\n",
    "\n",
    "---\n",
    "\n",
    "**Built with ‚ù§Ô∏è following Harvard NLP's educational materials**\n",
    "\n",
    "**This notebook demonstrates the clean, modular approach of the Annotated Transformer!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
