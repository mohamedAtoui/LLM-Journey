{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Multi-Head Attention (MHA) Transformer Training\n",
    "## FYP: Comparison of Transformer Attention Mechanisms\n",
    "\n",
    "This notebook trains a full encoder-decoder transformer with Multi-Head Attention on WikiText dataset.\n",
    "\n",
    "**Author:** Your Name  \n",
    "**Dataset:** WikiText-2  \n",
    "**Architecture:** Encoder-Decoder Transformer with MHA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive (for saving checkpoints)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone your repository\n",
    "# Replace with your actual repo URL\n",
    "!git clone https://github.com/YOUR_USERNAME/LLM-Journey.git\n",
    "%cd LLM-Journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets tensorboard matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_prep"
   },
   "source": [
    "## 2. Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_data"
   },
   "outputs": [],
   "source": [
    "# Check if data exists\n",
    "import os\n",
    "data_path = \"data_processed/wikitext2_processed\"\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"✓ Data found at {data_path}\")\n",
    "    !ls -lh {data_path}\n",
    "else:\n",
    "    print(f\"✗ Data not found at {data_path}\")\n",
    "    print(\"Please upload your processed data or run preprocessing script\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_config"
   },
   "outputs": [],
   "source": [
    "# Load and view configuration\n",
    "import json\n",
    "\n",
    "with open('mha/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"Current Configuration:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "modify_config"
   },
   "outputs": [],
   "source": [
    "# Optional: Modify configuration for Colab\n",
    "# (e.g., reduce batch size if running out of memory)\n",
    "\n",
    "# config['training_config']['batch_size'] = 16  # Reduce if OOM\n",
    "# config['training_config']['num_epochs'] = 5   # Adjust as needed\n",
    "\n",
    "# Update checkpoint and log directories to save to Google Drive\n",
    "config['logging_config']['checkpoint_dir'] = '/content/drive/MyDrive/LLM-Journey/checkpoints/mha'\n",
    "config['logging_config']['log_dir'] = '/content/drive/MyDrive/LLM-Journey/logs/mha'\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config['logging_config']['checkpoint_dir'], exist_ok=True)\n",
    "os.makedirs(config['logging_config']['log_dir'], exist_ok=True)\n",
    "\n",
    "# Save modified config\n",
    "with open('mha/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"✓ Configuration updated for Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model"
   },
   "source": [
    "## 4. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import training modules\n",
    "import sys\n",
    "sys.path.insert(0, 'mha')\n",
    "\n",
    "from transformer import Transformer\n",
    "from data_loader import WikiTextDataModule\n",
    "from utils import set_seed, count_parameters\n",
    "\n",
    "print(\"✓ Modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_model"
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "set_seed(config['random_seed'])\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = Transformer(\n",
    "    vocab_size=config['model_config']['vocab_size'],\n",
    "    d_model=config['model_config']['d_model'],\n",
    "    num_heads=config['model_config']['num_heads'],\n",
    "    num_encoder_layers=config['model_config']['num_encoder_layers'],\n",
    "    num_decoder_layers=config['model_config']['num_decoder_layers'],\n",
    "    d_ff=config['model_config']['d_ff'],\n",
    "    max_seq_length=config['model_config']['max_seq_length'],\n",
    "    dropout=config['model_config']['dropout'],\n",
    "    pe_type=config['positional_encoding']['type']\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_data"
   },
   "outputs": [],
   "source": [
    "# Initialize data module\n",
    "data_config = {\n",
    "    'train_path': config['data_config']['train_path'],\n",
    "    'val_path': config['data_config']['val_path'],\n",
    "    'batch_size': config['training_config']['batch_size'],\n",
    "    'max_seq_length': config['model_config']['max_seq_length'],\n",
    "    'tokenizer': config['data_config']['tokenizer']\n",
    "}\n",
    "\n",
    "data_module = WikiTextDataModule(data_config)\n",
    "data_module.setup()\n",
    "\n",
    "print(f\"\\n✓ Data loaded successfully\")\n",
    "print(f\"  Train samples: {len(data_module.train_dataset)}\")\n",
    "print(f\"  Val samples: {len(data_module.val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_tensorboard"
   },
   "outputs": [],
   "source": [
    "# Load TensorBoard (optional)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {config['logging_config']['log_dir']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from train import Trainer\n",
    "\n",
    "trainer = Trainer(config, device=device)\n",
    "trainer.train(num_epochs=config['training_config']['num_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 6. Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_best"
   },
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "best_checkpoint = os.path.join(config['logging_config']['checkpoint_dir'], 'best_model.pt')\n",
    "if os.path.exists(best_checkpoint):\n",
    "    checkpoint = torch.load(best_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"✓ Best model loaded\")\n",
    "    print(f\"  Val Loss: {checkpoint['metrics']['val_loss']:.4f}\")\n",
    "    print(f\"  Val Perplexity: {checkpoint['metrics']['val_ppl']:.2f}\")\n",
    "else:\n",
    "    print(\"Best checkpoint not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_attention"
   },
   "outputs": [],
   "source": [
    "# Visualize attention weights on a sample\n",
    "from utils import AttentionVisualizer\n",
    "from attention import create_combined_mask, create_padding_mask\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Get a sample batch\n",
    "val_loader = data_module.val_dataloader()\n",
    "batch = next(iter(val_loader))\n",
    "\n",
    "input_ids = batch['input_ids'].to(device)\n",
    "src = input_ids[:, :-1]\n",
    "tgt = input_ids[:, :-1]\n",
    "\n",
    "# Create masks\n",
    "src_mask = create_padding_mask(src, pad_token_id=0)\n",
    "tgt_mask = create_combined_mask(tgt, pad_token_id=0, causal=True)\n",
    "\n",
    "# Forward pass to get attention\n",
    "with torch.no_grad():\n",
    "    # You would need to modify the forward pass to return attention weights\n",
    "    # For now, this is a placeholder\n",
    "    output = model(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "print(\"\\nAttention visualization would go here\")\n",
    "print(\"(Requires modification to return attention weights from forward pass)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_text"
   },
   "outputs": [],
   "source": [
    "# Text generation (simple greedy decoding)\n",
    "def generate_text(model, tokenizer, prompt, max_length=50, device='cuda'):\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Get predictions\n",
    "            src = input_ids\n",
    "            tgt = input_ids\n",
    "            \n",
    "            output = model(src, tgt)\n",
    "            \n",
    "            # Get next token (greedy)\n",
    "            next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Append to sequence\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "            \n",
    "            # Stop if EOS token\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Test generation\n",
    "prompt = \"The transformer architecture\"\n",
    "generated = generate_text(model, data_module.tokenizer, prompt, max_length=30, device=device)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save"
   },
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_results"
   },
   "outputs": [],
   "source": [
    "# Results are automatically saved to Google Drive\n",
    "print(\"Training complete!\")\n",
    "print(f\"Checkpoints saved to: {config['logging_config']['checkpoint_dir']}\")\n",
    "print(f\"Logs saved to: {config['logging_config']['log_dir']}\")\n",
    "print(\"\\nYou can access these files in your Google Drive.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
