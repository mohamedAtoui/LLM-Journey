{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Multi-Head Attention (MHA) Transformer Training - Simplified\n## Train Transformer on WikiText-2\n\n**Simple, self-contained notebook - no complex modules needed!**\n\nThis notebook:\n- Loads **pre-processed WikiText-2** data (from Datasets.ipynb)\n- Trains encoder-decoder transformer with Multi-Head Attention\n- Optimized for Google Colab\n\n**Prerequisites:**\n1. Run `Datasets.ipynb` first to create processed data\n2. Download `data_processed.zip` from that notebook\n3. Upload it to Colab and extract: `!unzip data_processed.zip`\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected - training will be slow!\")\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Mount Google Drive (to save checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/mha_checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"‚úì Checkpoints will be saved to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repository\n",
    "!git clone https://github.com/YOUR_USERNAME/LLM-Journey.git\n",
    "%cd LLM-Journey\n",
    "\n",
    "print(\"‚úì Repository cloned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies AND the mha package properly!\n!pip install -q datasets transformers tqdm\n\n# Install YOUR mha package in editable mode (industry standard!)\n!pip install -q -e .\n\nprint(\"‚úì Packages installed!\")\nprint(\"‚úì MHA package installed in editable mode (proper way!)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5Ô∏è‚É£ Import Everything\n\n**Note:** We're using **proper Python package imports** (industry standard)!\n\nNo more `sys.path.insert(0, 'mha')` hacks. The package is properly installed with `pip install -e .` in the previous cell.\n\nThis means:\n- ‚úÖ Imports work from any directory\n- ‚úÖ Works reliably in Colab\n- ‚úÖ IDE autocomplete works\n- ‚úÖ Same as PyTorch, Transformers, etc.\n- ‚úÖ Professional & reproducible"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Standard imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.utils.data import DataLoader\nfrom transformers import GPT2Tokenizer\nfrom datasets import load_from_disk\nfrom tqdm import tqdm\nimport math\n\n# Import from YOUR package (properly installed!)\nfrom mha import Transformer\nfrom mha import create_combined_mask, create_padding_mask\n\nprint(\"‚úì All imports successful!\")\nprint(\"‚úì Using properly installed mha package (no sys.path hacks!)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Configuration (Simple Dictionary!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple configuration - adjust as needed\nconfig = {\n    # Model architecture\n    'vocab_size': 50257,         # GPT-2 tokenizer vocabulary\n    'd_model': 512,              # Model dimension\n    'num_heads': 8,              # Number of attention heads\n    'num_encoder_layers': 6,     # Encoder depth\n    'num_decoder_layers': 6,     # Decoder depth  \n    'd_ff': 2048,                # Feed-forward dimension\n    'max_seq_length': 512,       # Max sequence length (matches pre-processed data!)\n    'dropout': 0.1,              # Dropout probability\n    \n    # Training\n    'batch_size': 8,             # Batch size (small for Colab memory)\n    'num_epochs': 3,             # Number of epochs\n    'learning_rate': 0.0001,     # Peak learning rate\n    'warmup_steps': 2000,        # LR warmup steps\n    'gradient_clip': 1.0,        # Gradient clipping\n}\n\nprint(\"Configuration:\")\nfor k, v in config.items():\n    print(f\"  {k}: {v}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7Ô∏è‚É£ Load Pre-Processed Data (From Datasets.ipynb)\n\n**Note:** This loads the data you already prepared in `Datasets.ipynb`!\n\nBefore running this cell:\n1. Make sure you ran `Datasets.ipynb` and downloaded `data_processed.zip`\n2. Upload `data_processed.zip` to Colab\n3. Extract it: `!unzip data_processed.zip`\n\nThis will load the **already tokenized** WikiText-2 dataset (512 tokens, GPT-2 tokenizer)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_from_disk\n\n# Load pre-processed datasets (created from Datasets.ipynb)\nprint(\"Loading pre-processed WikiText-2 dataset...\")\nprint(\"(Make sure you've uploaded data_processed.zip and extracted it!)\\n\")\n\n# Path to your pre-processed data\nDATA_PATH = './data/wikitext2_processed'\n\ntry:\n    # Load from disk\n    dataset = load_from_disk(DATA_PATH)\n    \n    train_dataset = dataset['train']\n    val_dataset = dataset['validation']\n    \n    # Initialize tokenizer (still needed for pad_token_id)\n    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    # Set PyTorch format\n    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    \n    print(f\"‚úì Dataset loaded successfully!\")\n    print(f\"  Tokenizer vocab size: {len(tokenizer)}\")\n    print(f\"  Train samples: {len(train_dataset):,}\")\n    print(f\"  Val samples: {len(val_dataset):,}\")\n    print(f\"  Sequence length: {len(train_dataset[0]['input_ids'])} tokens\")\n    \nexcept FileNotFoundError:\n    print(\"‚ùå Error: Pre-processed data not found!\")\n    print(f\"\\nExpected path: {DATA_PATH}\")\n    print(\"\\nPlease:\")\n    print(\"  1. Run Datasets.ipynb to create the processed data\")\n    print(\"  2. Download data_processed.zip from Datasets.ipynb\")\n    print(\"  3. Upload and extract it to Colab:\")\n    print(\"     !unzip data_processed.zip\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Create DataLoaders (Simple!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple collate function (data is already in PyTorch format!)\ndef collate_fn(batch):\n    input_ids = torch.stack([item['input_ids'] for item in batch])\n    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'labels': input_ids.clone()  # For language modeling\n    }\n\n# Create dataloaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=config['batch_size'],\n    shuffle=True,\n    collate_fn=collate_fn\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=config['batch_size'],\n    shuffle=False,\n    collate_fn=collate_fn\n)\n\nprint(f\"‚úì DataLoaders created!\")\nprint(f\"  Train batches: {len(train_loader)}\")\nprint(f\"  Val batches: {len(val_loader)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer model\n",
    "model = Transformer(\n",
    "    vocab_size=config['vocab_size'],\n",
    "    d_model=config['d_model'],\n",
    "    num_heads=config['num_heads'],\n",
    "    num_encoder_layers=config['num_encoder_layers'],\n",
    "    num_decoder_layers=config['num_decoder_layers'],\n",
    "    d_ff=config['d_ff'],\n",
    "    max_seq_length=config['max_seq_length'],\n",
    "    dropout=config['dropout'],\n",
    "    pe_type='sinusoidal'  # Sinusoidal positional encoding\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úì Model created!\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Setup Optimizer, Scheduler & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9\n",
    ")\n",
    "\n",
    "# Learning rate scheduler with warmup (from original paper)\n",
    "def lr_lambda(step):\n",
    "    if step == 0:\n",
    "        return 0\n",
    "    d_model = config['d_model']\n",
    "    warmup = config['warmup_steps']\n",
    "    return (d_model ** -0.5) * min(step ** -0.5, step * warmup ** -1.5)\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Loss function (ignore padding tokens)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "print(\"‚úì Optimizer, scheduler, and loss function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, scheduler, criterion, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Get data\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Prepare src and tgt (shift by 1 for next-token prediction)\n",
    "        src = input_ids[:, :-1]          # All tokens except last\n",
    "        tgt_input = input_ids[:, :-1]    # Same (decoder input)\n",
    "        tgt_output = labels[:, 1:]       # All tokens except first (target)\n",
    "        \n",
    "        # Create masks\n",
    "        src_mask = create_padding_mask(src, pad_token_id=tokenizer.pad_token_id)\n",
    "        tgt_mask = create_combined_mask(tgt_input, pad_token_id=tokenizer.pad_token_id, causal=True)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(\n",
    "            output.reshape(-1, config['vocab_size']),\n",
    "            tgt_output.reshape(-1)\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip'])\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        num_tokens = (tgt_output != tokenizer.pad_token_id).sum().item()\n",
    "        total_loss += loss.item() * num_tokens\n",
    "        total_tokens += num_tokens\n",
    "        \n",
    "        # Update progress bar\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'lr': f\"{current_lr:.2e}\"\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        src = input_ids[:, :-1]\n",
    "        tgt_input = input_ids[:, :-1]\n",
    "        tgt_output = labels[:, 1:]\n",
    "        \n",
    "        src_mask = create_padding_mask(src, pad_token_id=tokenizer.pad_token_id)\n",
    "        tgt_mask = create_combined_mask(tgt_input, pad_token_id=tokenizer.pad_token_id, causal=True)\n",
    "        \n",
    "        output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "        \n",
    "        loss = criterion(\n",
    "            output.reshape(-1, config['vocab_size']),\n",
    "            tgt_output.reshape(-1)\n",
    "        )\n",
    "        \n",
    "        num_tokens = (tgt_output != tokenizer.pad_token_id).sum().item()\n",
    "        total_loss += loss.item() * num_tokens\n",
    "        total_tokens += num_tokens\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Main Training Loop üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "history = {'train_loss': [], 'train_ppl': [], 'val_loss': [], 'val_ppl': []}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting Training!\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for epoch in range(1, config['num_epochs'] + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch}/{config['num_epochs']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_ppl = train_epoch(\n",
    "        model, train_loader, optimizer, scheduler, criterion, device, epoch\n",
    "    )\n",
    "    print(f\"\\nüìä Train Loss: {train_loss:.4f} | Perplexity: {train_ppl:.2f}\")\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_ppl = validate(model, val_loader, criterion, device)\n",
    "    print(f\"üìä Val Loss: {val_loss:.4f} | Perplexity: {val_ppl:.2f}\")\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_ppl'].append(train_ppl)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_ppl'].append(val_ppl)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        checkpoint_path = f\"{CHECKPOINT_DIR}/best_model_epoch{epoch}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_ppl': val_ppl,\n",
    "            'config': config,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"‚úÖ Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "        print(f\"   Saved to: {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest Validation Loss: {best_val_loss:.4f}\")\n",
    "print(f\"Final Train PPL: {history['train_ppl'][-1]:.2f}\")\n",
    "print(f\"Final Val PPL: {history['val_ppl'][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£4Ô∏è‚É£ Text Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50, device='cuda'):\n",
    "    \"\"\"Generate text using greedy decoding\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            src = input_ids\n",
    "            tgt = input_ids\n",
    "            \n",
    "            # Create masks\n",
    "            src_mask = create_padding_mask(src, pad_token_id=tokenizer.pad_token_id)\n",
    "            tgt_mask = create_combined_mask(tgt, pad_token_id=tokenizer.pad_token_id, causal=True)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src, tgt, src_mask, tgt_mask)\n",
    "            \n",
    "            # Get next token (greedy)\n",
    "            next_token_logits = output[0, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            # Append to sequence\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "            \n",
    "            # Stop if EOS\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Test generation\n",
    "print(\"Testing text generation...\\n\")\n",
    "\n",
    "prompts = [\n",
    "    \"The transformer architecture\",\n",
    "    \"In the field of artificial intelligence\",\n",
    "    \"Machine learning is\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text(model, tokenizer, prompt, max_length=30, device=device)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 60)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£5Ô∏è‚É£ Load Best Model & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "import glob\n",
    "\n",
    "checkpoint_files = glob.glob(f\"{CHECKPOINT_DIR}/best_model_epoch*.pt\")\n",
    "\n",
    "if checkpoint_files:\n",
    "    latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
    "    print(f\"Loading best model: {latest_checkpoint}\")\n",
    "    \n",
    "    checkpoint = torch.load(latest_checkpoint)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"\\n‚úÖ Best model loaded!\")\n",
    "    print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"  Train Loss: {checkpoint['train_loss']:.4f}\")\n",
    "    print(f\"  Val Loss: {checkpoint['val_loss']:.4f}\")\n",
    "    print(f\"  Val Perplexity: {checkpoint['val_ppl']:.2f}\")\n",
    "    \n",
    "    # Test generation with best model\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Testing generation with best model:\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    prompt = \"The attention mechanism allows\"\n",
    "    generated = generate_text(model, tokenizer, prompt, max_length=40, device=device)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No checkpoint found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ All Done!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nCheckpoints saved at: {CHECKPOINT_DIR}\")\n",
    "print(\"You can find them in your Google Drive!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}