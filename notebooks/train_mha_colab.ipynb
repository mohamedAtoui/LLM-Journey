{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention (MHA) Transformer Training\n",
    "## Train Transformer on WikiText-2 - Harvard NLP Style\n",
    "\n",
    "**Based on Harvard NLP's Annotated Transformer** üéì\n",
    "\n",
    "This notebook:\n",
    "- Loads **pre-processed WikiText-2** data (from Datasets.ipynb)\n",
    "- Trains encoder-decoder transformer using **Harvard NLP's implementation patterns**\n",
    "- Optimized for Google Colab\n",
    "\n",
    "**What's special about this implementation?**\n",
    "- ‚úÖ Uses `make_model()` factory function (Harvard NLP way)\n",
    "- ‚úÖ Uses `rate()` for learning rate scheduling (from the paper)\n",
    "- ‚úÖ Uses `TextGenerator` for professional text generation\n",
    "- ‚úÖ Follows [Harvard NLP's Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Run `Datasets.ipynb` first to create processed data\n",
    "2. Download `data_processed.zip` from that notebook\n",
    "3. Upload it to Colab and extract: `!unzip data_processed.zip`\n",
    "\n",
    "**References:**\n",
    "- Harvard NLP Annotated Transformer: https://nlp.seas.harvard.edu/annotated-transformer/\n",
    "- Original Paper: \"Attention is All You Need\" (Vaswani et al., 2017)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected - training will be slow!\")\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Mount Google Drive (to save checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/mha_checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"‚úì Checkpoints will be saved to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repository\n",
    "!git clone https://github.com/mohamedAtoui/LLM-Journey\n",
    "%cd LLM-Journey\n",
    "\n",
    "print(\"‚úì Repository cloned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies AND the mha package properly!\n",
    "!pip install -q datasets transformers tqdm\n",
    "\n",
    "# Install YOUR mha package in editable mode (industry standard!)\n",
    "!pip install -q -e .\n",
    "\n",
    "print(\"‚úì Packages installed!\")\n",
    "print(\"‚úì MHA package installed in editable mode (proper way!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Import Everything - Harvard NLP Style\n",
    "\n",
    "**Note:** We're using **Harvard NLP's Annotated Transformer** patterns!\n",
    "\n",
    "The package is properly installed with `pip install -e .` in the previous cell.\n",
    "\n",
    "**What we're importing:**\n",
    "- ‚úÖ `make_model()`: Harvard NLP's factory function to create transformers\n",
    "- ‚úÖ `rate()`: Learning rate schedule from the paper\n",
    "- ‚úÖ `TextGenerator`: Professional text generation utilities\n",
    "- ‚úÖ `Batch`: Batch processing with automatic masking\n",
    "- ‚úÖ Mask utilities: For creating attention masks\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Imports work from any directory\n",
    "- ‚úÖ Works reliably in Colab\n",
    "- ‚úÖ IDE autocomplete works\n",
    "- ‚úÖ Same patterns as the original paper\n",
    "- ‚úÖ Professional & reproducible\n",
    "\n",
    "**Backward compatible:** The old `Transformer` class still works if you prefer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Harvard NLP style imports (RECOMMENDED)\n",
    "from mha import make_model\n",
    "from mha.utils import rate, Batch\n",
    "from mha.inference import TextGenerator\n",
    "from mha.attention import subsequent_mask\n",
    "\n",
    "# Mask utilities (for backward compatibility)\n",
    "from mha import create_combined_mask, create_padding_mask\n",
    "\n",
    "print(\"‚úì All imports successful!\")\n",
    "print(\"‚úì Using Harvard NLP's Annotated Transformer patterns!\")\n",
    "print(\"  - make_model() for creating transformers\")\n",
    "print(\"  - rate() for LR scheduling\")\n",
    "print(\"  - TextGenerator for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Configuration (Simple Dictionary!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harvard NLP style configuration\n",
    "config = {\n",
    "    # Model architecture (Harvard NLP parameter names)\n",
    "    'src_vocab': 50257,          # Source vocabulary size (GPT-2)\n",
    "    'tgt_vocab': 50257,          # Target vocabulary size (same for LM)\n",
    "    'N': 6,                      # Number of encoder/decoder layers\n",
    "    'd_model': 512,              # Model dimension\n",
    "    'd_ff': 2048,                # Feed-forward dimension\n",
    "    'h': 8,                      # Number of attention heads\n",
    "    'dropout': 0.1,              # Dropout probability\n",
    "    'max_seq_length': 512,       # Max sequence length (matches data!)\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    'batch_size': 8,             # Batch size (small for Colab memory)\n",
    "    'num_epochs': 3,             # Number of epochs\n",
    "    'warmup_steps': 2000,        # LR warmup steps (from paper)\n",
    "    'gradient_clip': 1.0,        # Gradient clipping\n",
    "}\n",
    "\n",
    "print(\"Configuration (Harvard NLP Style):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Model: make_model(src_vocab={config['src_vocab']}, tgt_vocab={config['tgt_vocab']}, N={config['N']})\")\n",
    "print(f\"  Dimensions: d_model={config['d_model']}, d_ff={config['d_ff']}, h={config['h']}\")\n",
    "print(f\"  Training: {config['num_epochs']} epochs, batch_size={config['batch_size']}\")\n",
    "print(f\"  LR Schedule: warmup_steps={config['warmup_steps']} (using rate() function)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Load Pre-Processed Data (From Datasets.ipynb)\n",
    "\n",
    "**Note:** This loads the data you already prepared in `Datasets.ipynb`!\n",
    "\n",
    "Before running this cell:\n",
    "1. Make sure you ran `Datasets.ipynb` and downloaded `data_processed.zip`\n",
    "2. Upload `data_processed.zip` to Colab\n",
    "3. Extract it: `!unzip data_processed.zip`\n",
    "\n",
    "This will load the **already tokenized** WikiText-2 dataset (512 tokens, GPT-2 tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load pre-processed datasets (created from Datasets.ipynb)\n",
    "print(\"Loading pre-processed WikiText-2 dataset...\")\n",
    "print(\"(Make sure you've uploaded data_processed.zip and extracted it!)\\n\")\n",
    "\n",
    "# Path to your pre-processed data\n",
    "DATA_PATH = './data/wikitext2_processed'\n",
    "\n",
    "try:\n",
    "    # Load from disk\n",
    "    dataset = load_from_disk(DATA_PATH)\n",
    "    \n",
    "    train_dataset = dataset['train']\n",
    "    val_dataset = dataset['validation']\n",
    "    \n",
    "    # Initialize tokenizer (still needed for pad_token_id)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Set PyTorch format\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "    \n",
    "    print(f\"‚úì Dataset loaded successfully!\")\n",
    "    print(f\"  Tokenizer vocab size: {len(tokenizer)}\")\n",
    "    print(f\"  Train samples: {len(train_dataset):,}\")\n",
    "    print(f\"  Val samples: {len(val_dataset):,}\")\n",
    "    print(f\"  Sequence length: {len(train_dataset[0]['input_ids'])} tokens\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: Pre-processed data not found!\")\n",
    "    print(f\"\\nExpected path: {DATA_PATH}\")\n",
    "    print(\"\\nPlease:\")\n",
    "    print(\"  1. Run Datasets.ipynb to create the processed data\")\n",
    "    print(\"  2. Download data_processed.zip from Datasets.ipynb\")\n",
    "    print(\"  3. Upload and extract it to Colab:\")\n",
    "    print(\"     !unzip data_processed.zip\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Create DataLoaders (Simple!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple collate function (data is already in PyTorch format!)\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': input_ids.clone()  # For language modeling\n",
    "    }\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"‚úì DataLoaders created!\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Initialize Model - Harvard NLP Style\n",
    "\n",
    "**Using `make_model()` factory function!**\n",
    "\n",
    "This is the standard way to create a Transformer from Harvard NLP's Annotated Transformer.\n",
    "\n",
    "**What `make_model()` does:**\n",
    "1. Creates encoder and decoder with N layers\n",
    "2. Initializes embeddings with ‚àöd_model scaling\n",
    "3. Adds sinusoidal positional encoding\n",
    "4. Creates generator (output projection)\n",
    "5. Applies Xavier/Glorot initialization\n",
    "\n",
    "**Comparison:**\n",
    "- **Old way:** `model = Transformer(vocab_size=..., d_model=..., num_heads=...)`\n",
    "- **Harvard NLP way:** `model = make_model(src_vocab=..., tgt_vocab=..., N=...)`\n",
    "\n",
    "Both work! But Harvard NLP's way is cleaner and matches the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer model using Harvard NLP's make_model() function\n",
    "model = make_model(\n",
    "    src_vocab=config['src_vocab'],\n",
    "    tgt_vocab=config['tgt_vocab'],\n",
    "    N=config['N'],\n",
    "    d_model=config['d_model'],\n",
    "    d_ff=config['d_ff'],\n",
    "    h=config['h'],\n",
    "    dropout=config['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úì Model created using make_model()!\")\n",
    "print(f\"  Architecture: EncoderDecoder with {config['N']} layers\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / 1e6:.1f} MB\")\n",
    "print(f\"\\n  Harvard NLP components:\")\n",
    "print(f\"    - Encoder: {config['N']}-layer stack\")\n",
    "print(f\"    - Decoder: {config['N']}-layer stack\")\n",
    "print(f\"    - Embeddings: with ‚àöd_model scaling\")\n",
    "print(f\"    - Positional Encoding: Sinusoidal\")\n",
    "print(f\"    - Generator: Linear + log_softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Setup Optimizer, Scheduler & Loss - Harvard NLP Style\n",
    "\n",
    "**Using `rate()` function for learning rate scheduling!**\n",
    "\n",
    "The `rate()` function implements the exact schedule from \"Attention is All You Need\":\n",
    "\n",
    "```\n",
    "lrate = d_model^(-0.5) * min(step^(-0.5), step * warmup^(-1.5))\n",
    "```\n",
    "\n",
    "This increases LR linearly during warmup, then decreases proportionally to the inverse square root of the step number.\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Exact formula from the paper\n",
    "- ‚úÖ Cleaner code (no manual lambda)\n",
    "- ‚úÖ Well-tested implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Adam optimizer (parameters from the paper)\noptimizer = optim.Adam(\n    model.parameters(),\n    lr=1.0,  # Will be scaled by rate() function\n    betas=(0.9, 0.98),\n    eps=1e-9\n)\n\n# Learning rate scheduler using Harvard NLP's rate() function\n# IMPORTANT: Start from step 1, not 0, to avoid division issues\nscheduler = LambdaLR(\n    optimizer,\n    lr_lambda=lambda step: rate(\n        step + 1,  # Add 1 to avoid step=0 issues\n        model_size=config['d_model'],\n        factor=1.0,\n        warmup=config['warmup_steps']\n    )\n)\n\n# Loss function - use NLLLoss since Generator outputs log_softmax (Harvard NLP way)\n# NLLLoss expects log probabilities, which is what model.generator() returns\ncriterion = nn.NLLLoss(ignore_index=tokenizer.pad_token_id, reduction='sum')\n\nprint(\"‚úì Optimizer, scheduler, and loss function ready!\")\nprint(f\"  Optimizer: Adam with betas=(0.9, 0.98), eps=1e-9\")\nprint(f\"  LR Schedule: rate() function with warmup={config['warmup_steps']}\")\nprint(f\"  Loss: NLLLoss with reduction='sum' (Harvard NLP way)\")\nprint(f\"\\n  LR at step 1: {rate(1, config['d_model'], 1.0, config['warmup_steps']):.6f}\")\nprint(f\"  LR at step {config['warmup_steps']}: {rate(config['warmup_steps'], config['d_model'], 1.0, config['warmup_steps']):.6f}\")\nprint(f\"  LR at step {config['warmup_steps']*2}: {rate(config['warmup_steps']*2, config['d_model'], 1.0, config['warmup_steps']):.6f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_epoch(model, train_loader, optimizer, scheduler, criterion, device, epoch):\n    \"\"\"Train for one epoch (Harvard NLP style with numerical stability)\"\"\"\n    model.train()\n    total_loss = 0\n    total_tokens = 0\n    \n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n    for batch_idx, batch in enumerate(pbar):\n        # Get data\n        input_ids = batch['input_ids'].to(device)\n        labels = batch['labels'].to(device)\n        \n        # Prepare src and tgt (shift by 1 for next-token prediction)\n        src = input_ids[:, :-1]          # All tokens except last\n        tgt_input = input_ids[:, :-1]    # Same (decoder input)\n        tgt_output = labels[:, 1:]       # All tokens except first (target)\n        \n        # Create masks\n        src_mask = create_padding_mask(src, pad_token_id=tokenizer.pad_token_id)\n        tgt_mask = create_combined_mask(tgt_input, pad_token_id=tokenizer.pad_token_id, causal=True)\n        \n        # Forward pass through model (returns decoder hidden states)\n        decoder_output = model(src, tgt_input, src_mask, tgt_mask)\n        \n        # Apply generator to get log probabilities (Harvard NLP way)\n        # Generator applies: linear projection + log_softmax\n        log_probs = model.generator(decoder_output)\n        \n        # Compute loss (NLLLoss with reduction='sum')\n        # We sum the loss and then divide by number of tokens (Harvard NLP way)\n        loss_sum = criterion(\n            log_probs.reshape(-1, config['tgt_vocab']),  # (batch*seq_len, vocab)\n            tgt_output.reshape(-1)                        # (batch*seq_len,)\n        )\n        \n        # Count non-padding tokens in this batch\n        num_tokens = (tgt_output != tokenizer.pad_token_id).sum().item()\n        \n        # Normalize loss by number of tokens\n        loss = loss_sum / num_tokens if num_tokens > 0 else loss_sum\n        \n        # Check for NaN/Inf before backward pass\n        if not torch.isfinite(loss):\n            print(f\"\\n‚ö†Ô∏è  Warning: Non-finite loss detected at batch {batch_idx}\")\n            print(f\"   Loss value: {loss.item()}\")\n            continue  # Skip this batch\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Gradient clipping (important for stability!)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip'])\n        \n        optimizer.step()\n        scheduler.step()\n        \n        # Track metrics (use loss_sum for proper averaging)\n        total_loss += loss_sum.item()\n        total_tokens += num_tokens\n        \n        # Update progress bar\n        current_lr = scheduler.get_last_lr()[0]\n        pbar.set_postfix({\n            'loss': f\"{loss.item():.4f}\",\n            'lr': f\"{current_lr:.2e}\"\n        })\n    \n    # Calculate average loss per token\n    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n    perplexity = math.exp(min(avg_loss, 100))  # Cap to prevent overflow\n    return avg_loss, perplexity"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef validate(model, val_loader, criterion, device):\n    \"\"\"Validate the model (Harvard NLP style with numerical stability)\"\"\"\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n    \n    for batch in tqdm(val_loader, desc=\"Validation\"):\n        input_ids = batch['input_ids'].to(device)\n        labels = batch['labels'].to(device)\n        \n        src = input_ids[:, :-1]\n        tgt_input = input_ids[:, :-1]\n        tgt_output = labels[:, 1:]\n        \n        src_mask = create_padding_mask(src, pad_token_id=tokenizer.pad_token_id)\n        tgt_mask = create_combined_mask(tgt_input, pad_token_id=tokenizer.pad_token_id, causal=True)\n        \n        # Forward pass through model (returns decoder hidden states)\n        decoder_output = model(src, tgt_input, src_mask, tgt_mask)\n        \n        # Apply generator to get log probabilities (Harvard NLP way)\n        log_probs = model.generator(decoder_output)\n        \n        # Compute loss (sum over batch)\n        loss_sum = criterion(\n            log_probs.reshape(-1, config['tgt_vocab']),  # (batch*seq_len, vocab)\n            tgt_output.reshape(-1)                        # (batch*seq_len,)\n        )\n        \n        num_tokens = (tgt_output != tokenizer.pad_token_id).sum().item()\n        total_loss += loss_sum.item()\n        total_tokens += num_tokens\n    \n    # Calculate average loss per token\n    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n    perplexity = math.exp(min(avg_loss, 100))  # Cap to prevent overflow\n    return avg_loss, perplexity"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Main Training Loop üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "history = {'train_loss': [], 'train_ppl': [], 'val_loss': [], 'val_ppl': []}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting Training!\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for epoch in range(1, config['num_epochs'] + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch}/{config['num_epochs']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_ppl = train_epoch(\n",
    "        model, train_loader, optimizer, scheduler, criterion, device, epoch\n",
    "    )\n",
    "    print(f\"\\nüìä Train Loss: {train_loss:.4f} | Perplexity: {train_ppl:.2f}\")\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_ppl = validate(model, val_loader, criterion, device)\n",
    "    print(f\"üìä Val Loss: {val_loss:.4f} | Perplexity: {val_ppl:.2f}\")\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_ppl'].append(train_ppl)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_ppl'].append(val_ppl)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        checkpoint_path = f\"{CHECKPOINT_DIR}/best_model_epoch{epoch}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_ppl': val_ppl,\n",
    "            'config': config,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"‚úÖ Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "        print(f\"   Saved to: {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest Validation Loss: {best_val_loss:.4f}\")\n",
    "print(f\"Final Train PPL: {history['train_ppl'][-1]:.2f}\")\n",
    "print(f\"Final Val PPL: {history['val_ppl'][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üéì Harvard NLP Features Showcase\n",
    "\n",
    "**Congratulations! You just trained a Transformer using Harvard NLP's Annotated Transformer patterns!**\n",
    "\n",
    "### What's Different?\n",
    "\n",
    "| Feature | Old Implementation | Harvard NLP Style (This Notebook) |\n",
    "|---------|-------------------|-----------------------------------|\n",
    "| **Model Creation** | `Transformer(vocab_size=..., d_model=...)` | `make_model(src_vocab=..., N=...)` |\n",
    "| **Architecture** | Manual initialization | Factory function with Xavier init |\n",
    "| **LR Schedule** | Custom lambda function | `rate()` function from paper |\n",
    "| **Generation** | Custom function | `TextGenerator` with multiple strategies |\n",
    "| **Parameter Names** | Mixed conventions | Paper's original names (N, h, d_ff) |\n",
    "\n",
    "### Key Harvard NLP Components Used\n",
    "\n",
    "1. **`make_model()`** - Factory function that creates:\n",
    "   - `EncoderDecoder` wrapper\n",
    "   - `Encoder` with N layers\n",
    "   - `Decoder` with N layers\n",
    "   - `Embeddings` with ‚àöd_model scaling\n",
    "   - `PositionalEncoding` (sinusoidal)\n",
    "   - `Generator` (output projection)\n",
    "\n",
    "2. **`rate()`** - Learning rate schedule:\n",
    "   ```python\n",
    "   lrate = d_model^(-0.5) * min(step^(-0.5), step * warmup^(-1.5))\n",
    "   ```\n",
    "\n",
    "3. **`TextGenerator`** - Professional generation with:\n",
    "   - Greedy decoding\n",
    "   - Temperature sampling  \n",
    "   - Top-k sampling\n",
    "   - Nucleus (top-p) sampling\n",
    "\n",
    "### Benefits\n",
    "\n",
    "‚úÖ **Cleaner Code**: Factory functions vs manual initialization  \n",
    "‚úÖ **Paper Accuracy**: Exact formulas from \"Attention is All You Need\"  \n",
    "‚úÖ **Educational**: Matches the annotated transformer tutorial  \n",
    "‚úÖ **Flexible**: Multiple generation strategies  \n",
    "‚úÖ **Professional**: Industry-standard patterns\n",
    "\n",
    "### Learn More\n",
    "\n",
    "- **Harvard NLP Annotated Transformer**: https://nlp.seas.harvard.edu/annotated-transformer/\n",
    "- **Original Paper**: \"Attention is All You Need\" (Vaswani et al., 2017)\n",
    "- **Your Implementation**: Modular package structure with Harvard NLP patterns\n",
    "\n",
    "**Backward Compatibility**: The old `Transformer` class still works! This notebook shows the recommended Harvard NLP way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£4Ô∏è‚É£ Text Generation - Harvard NLP Style\n",
    "\n",
    "**Using `TextGenerator` class!**\n",
    "\n",
    "The `TextGenerator` provides multiple sampling strategies:\n",
    "- **Greedy decoding**: Always pick the most likely token (fast, deterministic)\n",
    "- **Temperature sampling**: Control randomness with temperature parameter\n",
    "- **Top-k sampling**: Sample from top k most likely tokens\n",
    "- **Nucleus (top-p) sampling**: Sample from smallest set of tokens with cumulative probability > p\n",
    "\n",
    "This is much more flexible than a custom generation function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TextGenerator using Harvard NLP's class\n",
    "generator = TextGenerator(model, tokenizer, device=device)\n",
    "\n",
    "# Test generation with multiple strategies\n",
    "print(\"Testing Text Generation (Harvard NLP Style)...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "prompts = [\n",
    "    \"The transformer architecture\",\n",
    "    \"In the field of artificial intelligence\",\n",
    "    \"Machine learning is\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"\\n{i+1}. Prompt: \\\"{prompt}\\\"\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Greedy decoding (deterministic)\n",
    "    generated = generator.generate_greedy(prompt, max_length=30)\n",
    "    print(f\"   Greedy:      {generated}\")\n",
    "    \n",
    "    # Temperature sampling (more creative)\n",
    "    generated_temp = generator.generate_with_temperature(prompt, temperature=0.8, max_length=30)\n",
    "    print(f\"   Temperature: {generated_temp}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úì Text generation working!\")\n",
    "print(\"\\nAvailable methods:\")\n",
    "print(\"  - generate_greedy(): Deterministic, picks most likely token\")\n",
    "print(\"  - generate_with_temperature(): Controlled randomness\")\n",
    "print(\"  - generate_top_k(): Sample from top k tokens\")\n",
    "print(\"  - generate_nucleus(): Sample from cumulative probability > p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£5Ô∏è‚É£ Load Best Model & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "import glob\n",
    "\n",
    "checkpoint_files = glob.glob(f\"{CHECKPOINT_DIR}/best_model_epoch*.pt\")\n",
    "\n",
    "if checkpoint_files:\n",
    "    latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
    "    print(f\"Loading best model: {latest_checkpoint}\")\n",
    "    \n",
    "    checkpoint = torch.load(latest_checkpoint)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"\\n‚úÖ Best model loaded!\")\n",
    "    print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"  Train Loss: {checkpoint['train_loss']:.4f}\")\n",
    "    print(f\"  Val Loss: {checkpoint['val_loss']:.4f}\")\n",
    "    print(f\"  Val Perplexity: {checkpoint['val_ppl']:.2f}\")\n",
    "    \n",
    "    # Test generation with best model\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Testing generation with best model:\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    prompt = \"The attention mechanism allows\"\n",
    "    generated = generate_text(model, tokenizer, prompt, max_length=40, device=device)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No checkpoint found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ All Done!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nCheckpoints saved at: {CHECKPOINT_DIR}\")\n",
    "print(\"You can find them in your Google Drive!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}