# Technical Report: Multi-Head Attention (MHA) Transformer Implementation

**Project:** Comparison of Transformer Attention Mechanisms
**Author:** Your Name
**Date:** October 2025
**Implementation:** Multi-Head Attention (MHA) Baseline

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [System Architecture](#system-architecture)
3. [Module Documentation](#module-documentation)
4. [Mathematical Foundations](#mathematical-foundations)
5. [Implementation Details](#implementation-details)
6. [Testing and Verification](#testing-and-verification)
7. [Performance Characteristics](#performance-characteristics)
8. [Usage Guide](#usage-guide)
9. [Future Work](#future-work)

---

## 1. Executive Summary

This report documents the complete implementation of a Transformer Encoder-Decoder architecture with Multi-Head Attention (MHA) for language modeling. The implementation serves as the baseline for comparing various attention mechanisms (MQA, GQA, MLA) in the Final Year Project.

### Key Achievements

- âœ… Full encoder-decoder transformer with 6 layers each
- âœ… Multi-head attention with 8 heads (configurable)
- âœ… Scaled dot-product attention with masking support
- âœ… Sinusoidal and learned positional encodings
- âœ… Complete training pipeline with logging and checkpointing
- âœ… WikiText-2 data integration with 36,718 training samples
- âœ… ~44M parameters (512d model, 8 heads, 6 layers)

### Technology Stack

- **Framework:** PyTorch 2.0+
- **Dataset:** WikiText-2 (pre-tokenized with GPT-2 tokenizer)
- **Logging:** TensorBoard
- **Training:** Google Colab with GPU support
- **Version Control:** Git

---

## 2. System Architecture

### 2.1 High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT SEQUENCE                            â”‚
â”‚                    (Token IDs)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TOKEN EMBEDDING (vocab_size â†’ d_model)          â”‚
â”‚              POSITIONAL ENCODING (+ learned/sinusoidal)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ENCODER STACK (6 layers)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Layer 1-6:                                       â”‚      â”‚
â”‚  â”‚    â€¢ Multi-Head Self-Attention                    â”‚      â”‚
â”‚  â”‚    â€¢ Add & Norm (Residual + LayerNorm)           â”‚      â”‚
â”‚  â”‚    â€¢ Feed-Forward Network (FFN)                   â”‚      â”‚
â”‚  â”‚    â€¢ Add & Norm                                    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
                   Encoder Output
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECODER STACK (6 layers)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Layer 1-6:                                       â”‚      â”‚
â”‚  â”‚    â€¢ Masked Multi-Head Self-Attention             â”‚      â”‚
â”‚  â”‚    â€¢ Add & Norm                                    â”‚      â”‚
â”‚  â”‚    â€¢ Multi-Head Cross-Attention (to encoder)      â”‚      â”‚
â”‚  â”‚    â€¢ Add & Norm                                    â”‚      â”‚
â”‚  â”‚    â€¢ Feed-Forward Network                          â”‚      â”‚
â”‚  â”‚    â€¢ Add & Norm                                    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LINEAR PROJECTION (d_model â†’ vocab_size)        â”‚
â”‚              SOFTMAX (probability distribution)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
                   OUTPUT LOGITS
```

### 2.2 Module Organization

```
mha/
â”œâ”€â”€ config.json                 # Centralized configuration
â”œâ”€â”€ __init__.py                # Package initialization
â”‚
â”œâ”€â”€ Core Components:
â”‚   â”œâ”€â”€ layers.py              # Building blocks
â”‚   â”œâ”€â”€ positional_encoding.py # Position information
â”‚   â””â”€â”€ attention.py           # Attention mechanism
â”‚
â”œâ”€â”€ Model Architecture:
â”‚   â””â”€â”€ transformer.py         # Complete model
â”‚
â”œâ”€â”€ Data Pipeline:
â”‚   â””â”€â”€ data_loader.py         # Dataset handling
â”‚
â”œâ”€â”€ Training Infrastructure:
â”‚   â”œâ”€â”€ train.py               # Training loop
â”‚   â””â”€â”€ utils.py               # Utilities
â”‚
â””â”€â”€ Documentation:
    â”œâ”€â”€ README.md              # User guide
    â””â”€â”€ TECHNICAL_REPORT.md    # This document
```

---

## 3. Module Documentation

### 3.1 `config.json` - Configuration Management

**Purpose:** Centralized hyperparameter configuration for reproducibility and easy experimentation.

**Structure:**

```json
{
  "model_config": {
    "d_model": 512,              // Model dimension
    "num_heads": 8,              // Attention heads (d_model % num_heads == 0)
    "num_encoder_layers": 6,     // Encoder depth
    "num_decoder_layers": 6,     // Decoder depth
    "d_ff": 2048,               // FFN hidden dimension
    "dropout": 0.1,             // Dropout probability
    "max_seq_length": 512,      // Maximum sequence length
    "vocab_size": 50257         // GPT-2 vocabulary size
  },
  "positional_encoding": {
    "type": "sinusoidal",       // "sinusoidal" or "learned"
    "max_len": 5000             // Maximum position to encode
  },
  "training_config": {
    "batch_size": 32,           // Training batch size
    "learning_rate": 0.0001,    // Initial learning rate
    "num_epochs": 20,           // Training epochs
    "warmup_steps": 4000,       // LR warmup steps
    "gradient_clip": 1.0,       // Gradient clipping threshold
    "label_smoothing": 0.1,     // Label smoothing factor
    "optimizer": "adam",
    "adam_betas": [0.9, 0.98],  // Adam optimizer betas
    "adam_eps": 1e-9            // Adam epsilon
  },
  "data_config": {
    "dataset": "wikitext2",
    "train_path": "../data_processed/wikitext2_processed/train",
    "val_path": "../data_processed/wikitext2_processed/val",
    "tokenizer": "gpt2"
  },
  "logging_config": {
    "log_dir": "../logs/mha",
    "checkpoint_dir": "../checkpoints/mha",
    "save_every": 1000,         // Save checkpoint every N steps
    "log_every": 100,           // Log metrics every N steps
    "use_tensorboard": true,
    "use_wandb": false
  },
  "random_seed": 42
}
```

**Key Design Decisions:**

1. **d_model = 512, num_heads = 8:** Standard transformer configuration (d_k = 64 per head)
2. **d_ff = 2048:** 4Ã— expansion in FFN as per original paper
3. **warmup_steps = 4000:** Gradual learning rate increase prevents early instability
4. **label_smoothing = 0.1:** Prevents overconfidence, improves generalization

---

### 3.2 `layers.py` - Core Building Blocks

**Purpose:** Implements fundamental transformer components used throughout the architecture.

#### 3.2.1 LayerNorm

**Implementation:**
```python
class LayerNorm(nn.Module):
    def __init__(self, d_model: int, eps: float = 1e-6)
    def forward(self, x: Tensor) -> Tensor
```

**Mathematics:**
$$\text{LayerNorm}(x) = \gamma \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$$

Where:
- $\mu$ = mean across feature dimension
- $\sigma^2$ = variance across feature dimension
- $\gamma, \beta$ = learnable scale and shift parameters

**Shape Transformations:**
- Input: `(batch_size, seq_len, d_model)`
- Output: `(batch_size, seq_len, d_model)`

**Key Features:**
- Stabilizes training by normalizing layer inputs
- Learnable affine transformation
- Applied along feature dimension (not batch)

#### 3.2.2 FeedForward Network (FFN)

**Implementation:**
```python
class FeedForward(nn.Module):
    def __init__(self, d_model: int, d_ff: int, dropout: float, activation: str)
    def forward(self, x: Tensor) -> Tensor
```

**Mathematics:**
$$\text{FFN}(x) = \text{Dropout}(\text{Activation}(xW_1 + b_1))W_2 + b_2$$

**Architecture:**
```
Input (d_model)
    â†’ Linear (d_model â†’ d_ff)
    â†’ Activation (GELU/ReLU)
    â†’ Dropout
    â†’ Linear (d_ff â†’ d_model)
    â†’ Output (d_model)
```

**Shape Transformations:**
- Input: `(batch_size, seq_len, d_model)`
- Hidden: `(batch_size, seq_len, d_ff)`
- Output: `(batch_size, seq_len, d_model)`

**Activation Functions:**
- **GELU (default):** $\text{GELU}(x) = x \cdot \Phi(x)$ where $\Phi$ is Gaussian CDF
  - Smoother than ReLU, better for transformers
- **ReLU:** $\text{ReLU}(x) = \max(0, x)$
  - Faster, more memory efficient

**Parameter Count:**
- Layer 1: $d_{model} \times d_{ff} + d_{ff} = 512 \times 2048 + 2048 = 1,050,624$
- Layer 2: $d_{ff} \times d_{model} + d_{model} = 2048 \times 512 + 512 = 1,049,088$
- **Total per FFN:** ~2.1M parameters

#### 3.2.3 ResidualConnection

**Implementation:**
```python
class ResidualConnection(nn.Module):
    def __init__(self, d_model: int, dropout: float)
    def forward(self, x: Tensor, sublayer: Callable) -> Tensor
```

**Mathematics:**
$$\text{Output} = x + \text{Dropout}(\text{sublayer}(\text{LayerNorm}(x)))$$

**Purpose:**
- Enables gradient flow in deep networks
- Allows model to learn identity function easily
- Pre-norm variant used (layer norm before sublayer)

**Usage Pattern:**
```python
# In encoder/decoder layers
x = residual_connection(x, lambda x: attention(x, x, x, mask))
x = residual_connection(x, lambda x: feed_forward(x))
```

---

### 3.3 `positional_encoding.py` - Position Information

**Purpose:** Inject position information into token embeddings (transformers have no inherent position awareness).

#### 3.3.1 Sinusoidal Positional Encoding (Default)

**Mathematics:**
$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

Where:
- $pos$ = position in sequence (0 to seq_len-1)
- $i$ = dimension index (0 to d_model/2-1)
- Even dimensions use sine, odd use cosine

**Properties:**
- **Fixed (not learned):** Same for all sequences
- **Allows extrapolation:** Can handle longer sequences than seen during training
- **Relative position encoding:** $PE_{pos+k}$ can be represented as linear function of $PE_{pos}$

**Implementation:**
```python
class SinusoidalPositionalEncoding(nn.Module):
    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1)
    def forward(self, x: Tensor) -> Tensor
```

**Shape:**
- Input: `(batch_size, seq_len, d_model)`
- PE buffer: `(1, max_len, d_model)`
- Output: `(batch_size, seq_len, d_model)` (input + PE[:seq_len])

**Visualization (First 128 dimensions, 100 positions):**
```
Dim 0-1:   ~~~~~~~~~  (Low frequency)
Dim 2-3:   ~~~~~~~~   (Slightly higher)
...
Dim 126-127: |||||||| (High frequency)
```

#### 3.3.2 Learned Positional Encoding

**Implementation:**
```python
class LearnedPositionalEncoding(nn.Module):
    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1)
    def forward(self, x: Tensor) -> Tensor
```

**Properties:**
- **Learned:** Embedding layer with max_len positions
- **More flexible:** Can learn task-specific patterns
- **No extrapolation:** Cannot handle sequences longer than max_len

**Parameters:** $max\_len \times d_{model} = 5000 \times 512 = 2.56M$ parameters

**Trade-offs:**

| Feature | Sinusoidal | Learned |
|---------|-----------|---------|
| Parameters | 0 | 2.56M |
| Extrapolation | âœ… Yes | âŒ No |
| Task-specific | âŒ No | âœ… Yes |
| Training speed | âš¡ Faster | ğŸ¢ Slower |
| Memory | ğŸ’¾ Lower | ğŸ“¦ Higher |

---

### 3.4 `attention.py` - Multi-Head Attention Core

**Purpose:** Implements the heart of the transformer - the attention mechanism that allows the model to focus on relevant parts of the input.

#### 3.4.1 Scaled Dot-Product Attention

**Mathematics:**
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**Implementation:**
```python
class ScaledDotProductAttention(nn.Module):
    def forward(self, query, key, value, mask=None):
        # query: (batch, heads, seq_len_q, d_k)
        # key:   (batch, heads, seq_len_k, d_k)
        # value: (batch, heads, seq_len_v, d_k)

        scores = torch.matmul(query, key.transpose(-2, -1)) / sqrt(d_k)
        if mask: scores = scores.masked_fill(mask == 0, -1e9)
        weights = F.softmax(scores, dim=-1)
        output = torch.matmul(weights, value)
        return output, weights
```

**Step-by-Step Process:**

1. **Compute Attention Scores:** $QK^T$
   - Shape: `(batch, heads, seq_len_q, seq_len_k)`
   - Measures compatibility between query and all keys

2. **Scale by $\sqrt{d_k}$:**
   - Prevents softmax saturation with large d_k
   - With d_k=64: scaling factor = 8

3. **Apply Mask (if provided):**
   - Causal mask: prevents attending to future positions
   - Padding mask: prevents attending to padding tokens
   - Set masked positions to -1e9 (becomes ~0 after softmax)

4. **Softmax:** Convert scores to probabilities
   - Each row sums to 1
   - Shape: `(batch, heads, seq_len_q, seq_len_k)`

5. **Apply to Values:** $\text{weights} \times V$
   - Weighted sum of value vectors
   - Output shape: `(batch, heads, seq_len_q, d_k)`

**Why Scaling?**

Without scaling, for large d_k:
- Dot products grow large in magnitude
- Softmax gradients become very small
- Training becomes unstable

Example: If d_k=512 (no scaling), variance of $QK^T$ â‰ˆ 512, pushing softmax into regions with tiny gradients.

#### 3.4.2 Multi-Head Attention (MHA)

**Mathematics:**
$$\text{MultiHead}(Q, K, V) = \text{Concat}(head_1, ..., head_h)W^O$$
$$head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

**Architecture:**
```
Input (d_model=512)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Linear Projections:                      â”‚
â”‚    Q_proj: 512 â†’ 512  (W_q)              â”‚
â”‚    K_proj: 512 â†’ 512  (W_k)              â”‚
â”‚    V_proj: 512 â†’ 512  (W_v)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Split into 8 heads:                      â”‚
â”‚    (batch, seq, 512) â†’ (batch, 8, seq, 64)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scaled Dot-Product Attention (Ã—8 heads)  â”‚
â”‚    Each head attends independently        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Concatenate heads:                       â”‚
â”‚    (batch, 8, seq, 64) â†’ (batch, seq, 512)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output Projection:                       â”‚
â”‚    512 â†’ 512  (W_o)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
            Output (512)
```

**Implementation:**
```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model=512, num_heads=8, dropout=0.1):
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # 64

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, query, key, value, mask=None):
        # 1. Linear projections
        Q = self.W_q(query)  # (batch, seq_q, 512)
        K = self.W_k(key)    # (batch, seq_k, 512)
        V = self.W_v(value)  # (batch, seq_v, 512)

        # 2. Split into heads
        Q = self.split_heads(Q)  # (batch, 8, seq_q, 64)
        K = self.split_heads(K)  # (batch, 8, seq_k, 64)
        V = self.split_heads(V)  # (batch, 8, seq_v, 64)

        # 3. Scaled dot-product attention
        attn_output, weights = self.attention(Q, K, V, mask)

        # 4. Concatenate heads
        concat = self.combine_heads(attn_output)  # (batch, seq_q, 512)

        # 5. Output projection
        output = self.W_o(concat)
        return output, weights
```

**Why Multiple Heads?**

Different heads can learn to attend to different aspects:
- Head 1: Syntactic relationships (subject-verb)
- Head 2: Semantic relationships (entity-attribute)
- Head 3: Long-range dependencies
- ...

**Parameter Count per MHA:**
- Q projection: $512 \times 512 + 512 = 262,656$
- K projection: $512 \times 512 + 512 = 262,656$
- V projection: $512 \times 512 + 512 = 262,656$
- O projection: $512 \times 512 + 512 = 262,656$
- **Total:** 1,050,624 parameters

#### 3.4.3 Masking

**Causal Mask (Decoder Self-Attention):**
```python
def create_causal_mask(seq_len, device):
    # Lower triangular matrix
    mask = torch.tril(torch.ones(seq_len, seq_len))
    # Shape: (1, seq_len, seq_len)
```

**Example (seq_len=5):**
```
[[1, 0, 0, 0, 0],   # Position 0 can only see itself
 [1, 1, 0, 0, 0],   # Position 1 can see 0, 1
 [1, 1, 1, 0, 0],   # Position 2 can see 0, 1, 2
 [1, 1, 1, 1, 0],   # Position 3 can see 0, 1, 2, 3
 [1, 1, 1, 1, 1]]   # Position 4 can see all
```

**Padding Mask:**
```python
def create_padding_mask(seq, pad_token_id=0):
    # 1 for real tokens, 0 for padding
    mask = (seq != pad_token_id).unsqueeze(1)
    # Shape: (batch, 1, seq_len)
```

**Combined Mask:**
```python
def create_combined_mask(seq, pad_token_id=0, causal=True):
    # Combines causal + padding masks
    padding_mask = create_padding_mask(seq, pad_token_id)
    causal_mask = create_causal_mask(seq.size(1), seq.device)
    return padding_mask & causal_mask
```

---

### 3.5 `transformer.py` - Complete Architecture

**Purpose:** Assembles all components into a full encoder-decoder transformer.

#### 3.5.1 EncoderLayer

**Architecture:**
```
Input (d_model)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Multi-Head Self-Attention      â”‚
â”‚   Q=K=V (self-attention)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Add & Norm (Residual + LN)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feed-Forward Network           â”‚
â”‚   Linear â†’ GELU â†’ Linear       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Add & Norm                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
       Output (d_model)
```

**Implementation:**
```python
class EncoderLayer(nn.Module):
    def forward(self, x, mask=None):
        # Self-attention + residual
        x = self.residual1(x, lambda x: self.self_attention(x, x, x, mask)[0])
        # Feed-forward + residual
        x = self.residual2(x, self.feed_forward)
        return x
```

**Parameters per layer:** ~7.1M (4Ã— MHA + 2Ã— FFN)

#### 3.5.2 DecoderLayer

**Architecture:**
```
Input (d_model)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Masked Multi-Head Self-Attn    â”‚
â”‚   (Causal mask applied)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Add & Norm                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Multi-Head Cross-Attention     â”‚
â”‚   Q: from decoder              â”‚
â”‚   K,V: from encoder            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Add & Norm                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feed-Forward Network           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Add & Norm                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
       Output (d_model)
```

**Implementation:**
```python
class DecoderLayer(nn.Module):
    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):
        # Masked self-attention + residual
        x = self.residual1(x, lambda x: self.self_attention(x, x, x, tgt_mask)[0])
        # Cross-attention + residual
        x = self.residual2(x, lambda x: self.cross_attention(x, encoder_output,
                                                              encoder_output, src_mask)[0])
        # Feed-forward + residual
        x = self.residual3(x, self.feed_forward)
        return x
```

**Parameters per layer:** ~9.6M (6Ã— MHA + 2Ã— FFN)

#### 3.5.3 Full Transformer

**Complete Forward Pass:**

```python
class Transformer(nn.Module):
    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        # 1. Embed tokens
        src_embedded = self.src_embedding(src) * sqrt(d_model)
        tgt_embedded = self.tgt_embedding(tgt) * sqrt(d_model)

        # 2. Add positional encoding
        src_embedded = self.src_pos_encoding(src_embedded)
        tgt_embedded = self.tgt_pos_encoding(tgt_embedded)

        # 3. Encode
        encoder_output = self.encoder(src_embedded, src_mask)

        # 4. Decode
        decoder_output = self.decoder(tgt_embedded, encoder_output,
                                      src_mask, tgt_mask)

        # 5. Project to vocabulary
        output = self.output_projection(decoder_output)
        return output  # (batch, seq_len, vocab_size)
```

**Total Model Parameters:**

| Component | Parameters |
|-----------|------------|
| Token Embeddings (Ã—2) | 50,257 Ã— 512 Ã— 2 = 51.5M |
| Positional Encoding | 0 (sinusoidal) |
| Encoder (6 layers) | 6 Ã— 7.1M = 42.6M |
| Decoder (6 layers) | 6 Ã— 9.6M = 57.6M |
| Output Projection | 512 Ã— 50,257 = 25.7M |
| **Total** | **~177M parameters** |

Note: Actual may vary based on shared embeddings/projections.

---

### 3.6 `data_loader.py` - Data Pipeline

**Purpose:** Load and batch pre-processed WikiText data for training.

#### 3.6.1 WikiTextDataset

**Data Format:**
```python
# Pre-processed HuggingFace dataset format
{
    'input_ids': List[int],      # Token IDs
    'attention_mask': List[int]  # 1=real, 0=padding
}
```

**Processing:**
1. Load from disk using `datasets.load_from_disk()`
2. Truncate sequences exceeding max_seq_length
3. Convert to PyTorch tensors
4. Return as dictionary

#### 3.6.2 WikiTextDataModule

**Collate Function:**
```python
def collate_fn(self, batch):
    # Find max length in batch
    max_len = max(item['input_ids'].size(0) for item in batch)

    # Pad all sequences to max_len
    for item in batch:
        padding_len = max_len - len(item['input_ids'])
        # Pad with pad_token_id

    # Stack into batch tensors
    return {
        'input_ids': ...,      # (batch, max_len)
        'attention_mask': ..., # (batch, max_len)
        'labels': ...          # Same as input_ids for LM
    }
```

**Data Statistics (WikiText-2):**
- Training samples: 36,718
- Validation samples: 3,760
- Test samples: 4,358
- Average sequence length: ~300 tokens
- Vocabulary size: 50,257 (GPT-2 tokenizer)

---

### 3.7 `utils.py` - Training Utilities

#### 3.7.1 MetricsTracker

**Purpose:** Track loss and perplexity during training.

**Metrics:**
```python
Average Loss = Î£(loss Ã— num_tokens) / Î£(num_tokens)
Perplexity = exp(Average Loss)
```

**Interpretation:**
- Lower perplexity = better model
- Perplexity â‰ˆ weighted branching factor
- WikiText-2 baseline perplexity: ~80-120

#### 3.7.2 LabelSmoothing

**Mathematics:**
$$\text{smoothed\_target}_i = \begin{cases}
1 - \epsilon & \text{if } i = y \\
\epsilon / (|V| - 1) & \text{otherwise}
\end{cases}$$

**Benefits:**
- Prevents overconfidence
- Improves generalization
- Reduces overfitting

**Default:** Îµ = 0.1 (90% confidence on true label)

#### 3.7.3 Logger

**TensorBoard Integration:**
```python
logger.log_scalar('train/loss', loss, step)
logger.log_scalar('train/perplexity', ppl, step)
logger.log_scalar('train/learning_rate', lr, step)
```

**View logs:**
```bash
tensorboard --logdir ../logs/mha
```

#### 3.7.4 CheckpointManager

**Checkpoint Contents:**
```python
{
    'epoch': int,
    'step': int,
    'model_state_dict': OrderedDict,
    'optimizer_state_dict': dict,
    'metrics': {'val_loss': float, 'val_ppl': float}
}
```

**Auto-management:**
- Keeps only last 5 checkpoints
- Always saves `best_model.pt`
- Saves every N steps (configurable)

#### 3.7.5 AttentionVisualizer

**Visualizations:**
1. Single head heatmap
2. Multi-head comparison
3. Layer-wise attention patterns

**Usage:**
```python
AttentionVisualizer.plot_attention_heatmap(
    attention_weights,  # (batch, heads, seq, seq)
    src_tokens=['the', 'cat', 'sat'],
    tgt_tokens=['le', 'chat'],
    head_idx=0
)
```

---

### 3.8 `train.py` - Training Pipeline

#### 3.8.1 Trainer Class

**Complete Training Loop:**

```python
class Trainer:
    def __init__(self, config, device='cuda'):
        # 1. Initialize components
        self.model = build_model()
        self.optimizer = build_optimizer()
        self.scheduler = build_scheduler()
        self.criterion = LabelSmoothing(...)

    def train_epoch(self, epoch):
        for batch in train_loader:
            # Forward pass
            output = self.model(src, tgt, src_mask, tgt_mask)

            # Compute loss
            loss = self.criterion(output, labels)

            # Backward pass
            loss.backward()
            clip_grad_norm_(model.parameters(), max_norm)
            optimizer.step()
            scheduler.step()

            # Log metrics
            if step % log_every == 0:
                logger.log_metrics(...)

            # Save checkpoint
            if step % save_every == 0:
                checkpoint_manager.save(...)

    def validate(self):
        with torch.no_grad():
            for batch in val_loader:
                output = self.model(...)
                loss = self.criterion(...)
                tracker.update(loss)
        return avg_loss, perplexity

    def train(self, num_epochs):
        for epoch in range(num_epochs):
            train_loss = self.train_epoch(epoch)
            val_loss = self.validate()

            # Save best model
            if val_loss < best_val_loss:
                save_checkpoint('best_model.pt')
```

#### 3.8.2 Learning Rate Schedule

**Warmup + Inverse Square Root:**

$$lr = d_{model}^{-0.5} \cdot \min(step^{-0.5}, step \cdot warmup\_steps^{-1.5})$$

**Behavior:**
- Steps 0-4000: Linear warmup from 0 to peak
- Steps 4000+: Inverse square root decay

**Visualization:**
```
LR
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â•²
â”‚  â•±           â•²___
â”‚ â•±                 â•²___
â”‚â•±                       â•²___
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Steps
 0   4k   8k   12k   16k
```

**Why this schedule?**
- Early warmup prevents gradient explosion
- Gradual decay allows fine-tuning
- Matches original transformer paper

---

## 4. Mathematical Foundations

### 4.1 Attention Mechanism

**Intuition:** Query asks "What should I look at?", Keys say "This is what I contain", Values provide the actual information.

**Example:**
```
Sentence: "The cat sat on the mat"
Position 2 (sat) queries:
  - High attention to "cat" (subject)
  - Medium attention to "on" (preposition)
  - Low attention to "The" (determiner)
```

**Self-Attention Properties:**
- Permutation equivariant (without PE)
- O(nÂ²) complexity in sequence length
- Parallelizable across sequence

### 4.2 Why Encoder-Decoder?

**Encoder:**
- Bidirectional attention (sees full context)
- Builds rich representations
- Used for understanding

**Decoder:**
- Unidirectional attention (causal)
- Generates output autoregressively
- Used for generation

**For Language Modeling:**
- Source = Target (shifted by 1)
- Encoder provides context understanding
- Decoder generates next token predictions

### 4.3 Computational Complexity

**Per Layer:**
| Operation | Complexity | Parameters |
|-----------|------------|------------|
| Self-Attention | O(nÂ²d) | 4dÂ² |
| FFN | O(ndÂ²) | 8dÂ² |
| **Total** | **O(nÂ²d + ndÂ²)** | **12dÂ²** |

Where:
- n = sequence length
- d = model dimension

**Memory:**
- Attention weights: O(nÂ² Ã— heads)
- Activations: O(n Ã— d Ã— layers)

---

## 5. Implementation Details

### 5.1 Initialization

**Xavier/Glorot Uniform:**
```python
def _init_weights(self):
    for p in self.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform_(p)
```

**Why:** Maintains variance across layers, prevents vanishing/exploding gradients.

### 5.2 Dropout Strategy

**Locations:**
1. After attention weights
2. After FFN activation
3. After positional encoding
4. In residual connections

**Rate:** 0.1 (10% of neurons dropped)

### 5.3 Numerical Stability

**Techniques Used:**
1. **Attention scaling:** Division by âˆšd_k
2. **Gradient clipping:** Max norm = 1.0
3. **Label smoothing:** Îµ = 0.1
4. **Layer normalization:** Îµ = 1e-6
5. **Adam epsilon:** 1e-9

### 5.4 Memory Optimization

**Techniques:**
1. Gradient accumulation (if needed)
2. Mixed precision training (fp16)
3. Gradient checkpointing (for very deep models)
4. Efficient attention implementations

---

## 6. Testing and Verification

### 6.1 Unit Tests

**All modules include built-in tests:**

```bash
python layers.py
âœ“ All layer tests passed!

python positional_encoding.py
âœ“ All positional encoding tests passed!

python attention.py
âœ“ All Multi-Head Attention tests passed!
âœ“ Attention weights sum to 1.0

python transformer.py
âœ“ All Transformer tests passed!

python data_loader.py
âœ“ All data loader tests passed!
```

### 6.2 Attention Verification

**Critical Test:**
```python
# Verify attention weights sum to 1
weights_sum = attn_weights.sum(dim=-1)
assert torch.allclose(weights_sum, torch.ones_like(weights_sum), atol=1e-6)
âœ“ PASSED
```

**Interpretation:** Each query position distributes exactly 1.0 probability across all key positions.

### 6.3 Shape Consistency

**Verified transformations:**
```python
Input:     (batch=2, seq=10, d_model=512)
Embedding: (batch=2, seq=10, d_model=512) âœ“
Encoder:   (batch=2, seq=10, d_model=512) âœ“
Decoder:   (batch=2, seq=8, d_model=512) âœ“
Output:    (batch=2, seq=8, vocab=50257) âœ“
```

### 6.4 Integration Test

**Full forward pass:**
```python
src = torch.randint(0, vocab_size, (batch_size, seq_len_src))
tgt = torch.randint(0, vocab_size, (batch_size, seq_len_tgt))
output = model(src, tgt)
assert output.shape == (batch_size, seq_len_tgt, vocab_size)
âœ“ PASSED
```

---

## 7. Performance Characteristics

### 7.1 Model Size

**Configuration:**
- d_model: 512
- num_heads: 8
- num_layers: 6 (encoder) + 6 (decoder)
- vocab_size: 50,257

**Total Parameters:** ~177M (varies with implementation details)

**Comparison:**
- GPT-2 Small: 117M
- BERT Base: 110M
- T5 Small: 60M
- **This implementation: 177M** (encoder-decoder)

### 7.2 Training Speed

**Expected Performance (V100 GPU):**
- Batch size 32: ~5 batches/sec
- Epoch time (WikiText-2): ~20 minutes
- Full training (20 epochs): ~6-7 hours

**Memory Usage:**
- Model: ~700 MB
- Batch (32, 512): ~1.5 GB
- Optimizer state: ~1.4 GB
- **Total:** ~3.5 GB

**Optimizations for Colab (T4 GPU):**
- Reduce batch size to 16 if OOM
- Enable gradient accumulation
- Use mixed precision (fp16)

### 7.3 Inference Speed

**Text Generation:**
- Greedy decoding: ~10-20 tokens/sec
- Beam search (k=5): ~3-5 tokens/sec

**Bottlenecks:**
- Autoregressive generation (sequential)
- Cross-attention to full encoder output
- Large vocabulary softmax

---

## 8. Usage Guide

### 8.1 Local Testing

**Test individual components:**
```bash
cd mha
python layers.py
python attention.py
python transformer.py
```

**Test data loading:**
```bash
python data_loader.py
# Requires: ../data_processed/wikitext2_processed/
```

### 8.2 Local Training

**Basic training:**
```bash
cd mha
python train.py --config config.json
```

**Resume from checkpoint:**
```bash
python train.py --config config.json --checkpoint ../checkpoints/mha/checkpoint_epoch5_step1000.pt
```

**Monitor with TensorBoard:**
```bash
tensorboard --logdir ../logs/mha
# Open http://localhost:6006
```

### 8.3 Google Colab Training

**Steps:**

1. **Setup:**
   ```python
   # In notebook cell 1
   from google.colab import drive
   drive.mount('/content/drive')

   !git clone https://github.com/YOUR_USERNAME/LLM-Journey.git
   %cd LLM-Journey
   !pip install -r requirements.txt
   ```

2. **Configuration:**
   ```python
   # Update paths to save to Google Drive
   config['logging_config']['checkpoint_dir'] = '/content/drive/MyDrive/checkpoints/mha'
   ```

3. **Train:**
   ```python
   from mha.train import Trainer
   trainer = Trainer(config)
   trainer.train()
   ```

4. **Results:**
   - Checkpoints: Google Drive
   - Logs: TensorBoard in notebook
   - Best model: `best_model.pt`

### 8.4 Hyperparameter Tuning

**Key hyperparameters to tune:**

| Parameter | Range | Impact |
|-----------|-------|--------|
| learning_rate | 1e-5 to 5e-4 | Convergence speed |
| warmup_steps | 1000-8000 | Early stability |
| batch_size | 8-64 | Training speed/memory |
| dropout | 0.0-0.3 | Regularization |
| num_heads | 4, 8, 16 | Model capacity |
| d_model | 256, 512, 768 | Model size |
| label_smoothing | 0.0-0.2 | Generalization |

**Recommended tuning order:**
1. Learning rate (most critical)
2. Warmup steps
3. Dropout
4. Label smoothing
5. Model architecture (d_model, num_heads)

---

## 9. Future Work

### 9.1 Immediate Extensions (FYP)

**Other Attention Mechanisms:**

1. **Multi-Query Attention (MQA):**
   - Single K,V shared across all heads
   - Much faster inference
   - File: `mqa/attention.py`

2. **Grouped Query Attention (GQA):**
   - Groups of queries share K,V
   - Balance between MHA and MQA
   - File: `gqa/attention.py`

3. **Multi-Head Latent Attention (MLA):**
   - Compress K,V into latent space
   - Reduced memory footprint
   - File: `mla/attention.py`

### 9.2 Performance Improvements

**Optimizations:**
- Flash Attention integration
- Kernel fusion for FFN
- KV cache for inference
- Quantization (int8/fp16)

### 9.3 Advanced Features

**Extensions:**
- Relative positional encoding
- Sparse attention patterns
- ALiBi positional bias
- Rotary embeddings (RoPE)

### 9.4 Evaluation Metrics

**Add comprehensive evaluation:**
- Token-level accuracy
- BLEU scores (if doing translation)
- Attention pattern visualization
- Speed benchmarking (tokens/sec)
- Memory profiling

### 9.5 Comparison Framework

**For FYP comparison:**

```python
# comparison/
â”œâ”€â”€ compare_attention.py    # Side-by-side comparison
â”œâ”€â”€ benchmark.py           # Speed/memory benchmarks
â”œâ”€â”€ visualize.py          # Attention pattern analysis
â””â”€â”€ results.json          # Aggregated results
```

**Metrics to compare:**
- Perplexity (quality)
- Training speed (efficiency)
- Inference speed
- Memory usage
- Parameter count
- Attention pattern differences

---

## 10. Conclusion

### 10.1 Achievements

This implementation provides:
âœ… **Complete MHA transformer** - Fully functional encoder-decoder
âœ… **Production-ready code** - Proper logging, checkpointing, testing
âœ… **Modular design** - Easy to extend for other attention mechanisms
âœ… **Comprehensive documentation** - Every component explained
âœ… **Verified correctness** - All tests passing

### 10.2 Key Strengths

1. **Educational:** Clear code with extensive comments and dimension annotations
2. **Extensible:** Easy to swap attention mechanisms for comparison
3. **Reproducible:** Fixed seeds, deterministic training
4. **Practical:** Works on both local machines and Google Colab

### 10.3 Lessons Learned

1. **Attention is powerful but expensive:** O(nÂ²) complexity limits sequence length
2. **Scaling matters:** âˆšd_k scaling is critical for stability
3. **Residual connections are essential:** Enable training deep networks
4. **Proper initialization matters:** Xavier init prevents gradient issues
5. **Testing is crucial:** Unit tests caught several shape mismatches

### 10.4 Next Steps for FYP

1. âœ… Complete MHA implementation (Week 3)
2. â³ Implement MQA (Week 4)
3. â³ Implement GQA (Week 5)
4. â³ Implement MLA (Week 6)
5. â³ Comparative analysis (Week 7)
6. â³ Write final report (Week 8)

---

## Appendices

### Appendix A: File Manifest

```
mha/
â”œâ”€â”€ __init__.py              [12 lines]   Package initialization
â”œâ”€â”€ config.json              [48 lines]   Configuration
â”œâ”€â”€ layers.py                [215 lines]  Core layers
â”œâ”€â”€ positional_encoding.py   [248 lines]  Positional encodings
â”œâ”€â”€ attention.py             [334 lines]  Multi-head attention
â”œâ”€â”€ transformer.py           [338 lines]  Full architecture
â”œâ”€â”€ data_loader.py           [226 lines]  Data pipeline
â”œâ”€â”€ utils.py                 [358 lines]  Training utilities
â”œâ”€â”€ train.py                 [267 lines]  Training script
â”œâ”€â”€ README.md                [182 lines]  User documentation
â””â”€â”€ TECHNICAL_REPORT.md      [THIS FILE]  Technical documentation

Total: ~2,200+ lines of code
```

### Appendix B: Dependencies

```
torch >= 2.0.0
transformers >= 4.30.0
datasets >= 2.12.0
numpy >= 1.24.0
tensorboard >= 2.13.0
matplotlib >= 3.7.0
seaborn >= 0.12.0
tqdm >= 4.65.0
```

### Appendix C: References

1. Vaswani et al. (2017) - "Attention Is All You Need"
2. Ba et al. (2016) - Layer Normalization
3. Hendrycks & Gimpel (2016) - GELU activation
4. Szegedy et al. (2016) - Label smoothing
5. Merity et al. (2017) - WikiText datasets

---

**End of Technical Report**

*This document is part of the Final Year Project: "Comparison of Transformer Attention Mechanisms"*
*For questions or clarifications, please refer to the code comments or contact the author.*
