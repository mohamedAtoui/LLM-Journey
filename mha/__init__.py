"""
Multi-Head Attention (MHA) Implementation
Standard transformer attention mechanism from "Attention Is All You Need"
"""

__version__ = "1.0.0"
__author__ = "Your Name"
