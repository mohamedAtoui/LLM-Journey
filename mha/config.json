{
  "model_name": "mha_transformer",
  "architecture": "encoder_decoder",

  "model_config": {
    "d_model": 512,
    "num_heads": 8,
    "num_encoder_layers": 6,
    "num_decoder_layers": 6,
    "d_ff": 2048,
    "dropout": 0.1,
    "max_seq_length": 512,
    "vocab_size": 50257
  },

  "positional_encoding": {
    "type": "sinusoidal",
    "max_len": 5000
  },

  "training_config": {
    "batch_size": 32,
    "learning_rate": 0.0001,
    "num_epochs": 20,
    "warmup_steps": 4000,
    "gradient_clip": 1.0,
    "label_smoothing": 0.1,
    "optimizer": "adam",
    "adam_betas": [0.9, 0.98],
    "adam_eps": 1e-9
  },

  "data_config": {
    "dataset": "wikitext2",
    "train_path": "../data_processed/wikitext2_processed/train",
    "val_path": "../data_processed/wikitext2_processed/validation",
    "tokenizer": "gpt2"
  },

  "logging_config": {
    "log_dir": "../logs/mha",
    "checkpoint_dir": "../checkpoints/mha",
    "save_every": 1000,
    "log_every": 100,
    "use_tensorboard": true,
    "use_wandb": false
  },

  "random_seed": 42
}
